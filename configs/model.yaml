# Model logic analysis for THREADS reproduction.
# Purpose: declare encoder architecture and dimensional contracts used by
# ThreadsModel and training modules, strictly aligned to paper text + config.yaml.

model:
  version: "1.0"

  threads:
    name: "THREADS"
    output_dim: 1024

    multimodal_objective:
      type: "cross-modal contrastive learning (WSI <-> molecular)"
      pairing_policy:
        - "Use matched WSI-RNA pairs when RNA modality is available."
        - "Use matched WSI-DNA pairs when DNA modality is available."
      temperature: null  # Not specified in config.yaml
      implementation_note: "ThreadsModel.contrastive_loss must accept WSI embedding and one molecular embedding tensor per batch path."

    slide_encoder:
      type: "ABMIL gated attention"
      input_dim: 768
      hidden_dim: 1024
      num_heads: 2
      output_dim: 1024
      pre_attention_dropout: 0.1
      attention_dropout: 0.25
      activation: "GELU"

      architecture_contract:
        pre_attention_network:
          - "Project patch features from 768 to 1024."
          - "Use three hidden layers with LayerNorm and GELU as described in paper text."
          - "Apply dropout with pre_attention_dropout=0.1."
        gated_attention_block:
          - "Use gated attention with parallel tanh and sigmoid branches, then element-wise product."
          - "Attention hidden dimension remains 1024."
          - "Apply dropout with attention_dropout=0.25 in attention branches."
        multihead_aggregation:
          - "Chunk projected features into num_heads streams."
          - "Pool each head with gated attention."
          - "Concatenate head-wise pooled vectors."
          - "Apply post-attention linear projection back to output_dim=1024."

      tensor_shapes:
        patch_features: "[B, N, 768]"
        patch_mask: "[B, N]"
        head_pooled_before_concat: "[B, 1024] per head"
        concatenated_heads: "[B, num_heads * 1024]"
        slide_embedding: "[B, 1024]"

      interface_binding:
        class_path: "src/models/slide/threads_slide_encoder.py"
        constructor: "ThreadsSlideEncoder(in_dim, hidden_dim, n_heads, out_dim, dropout)"
        forward_signature: "forward(patch_feats, patch_mask) -> Tensor"

    rna_encoder:
      backbone: "scGPT (pancancer checkpoint)"
      transformer_layers: 12
      attention_heads: 8
      gene_embedding_dim: 512
      expr_encoder_dropout: 0.2
      projection_out_dim: 1024
      fine_tune_all_layers: true

      input_contract:
        gene_ids: "Integer tensor of gene token ids; must be in scGPT vocabulary."
        expr_values: "Float tensor of expression values; preprocessing is log2(TPM) per data config."

      architecture_contract:
        - "Use scGPT gene-identity encoder + expression-value encoder + transformer stack."
        - "Bypass decoder and mean-pool token outputs (including CLS) from final transformer layer."
        - "Project pooled 512-d representation to projection_out_dim=1024."
        - "All encoder layers are trainable during pretraining."

      tensor_shapes:
        gene_ids: "[B, G]"
        expr_values: "[B, G]"
        pooled_transformer: "[B, 512]"
        rna_embedding: "[B, 1024]"

      interface_binding:
        class_path: "src/models/encoders/rna_scgpt_encoder.py"
        constructor: "RnaScgptEncoder(ckpt_path, out_dim)"
        forward_signature: "forward(gene_ids, expr_values) -> Tensor"

    dna_encoder:
      input_dim: 1673
      hidden_dim: 1673
      layers: 4
      activation: "ReLU"
      dropout: 0.2
      output_dim: 1024

      input_contract:
        variant_vector: "Multi-hot encoded variant vector length 1673 (239 genes x 7 status bins)."

      architecture_contract:
        - "Use 4-layer MLP (input, two hidden, output) with ReLU activation."
        - "Hidden width is fixed to hidden_dim=1673."
        - "Apply dropout=0.2 between hidden layers."
        - "Final projection output_dim must be 1024."

      tensor_shapes:
        variant_vector: "[B, 1673]"
        dna_embedding: "[B, 1024]"

      interface_binding:
        class_path: "src/models/encoders/dna_mlp_encoder.py"
        constructor: "DnaMlpEncoder(in_dim, hidden_dim, out_dim, dropout)"
        forward_signature: "forward(variant_vector) -> Tensor"

    composed_model_contract:
      class_path: "src/models/threads_model.py"
      constructor: "ThreadsModel(slide_encoder, rna_encoder, dna_encoder, temperature)"
      public_methods:
        - "encode_wsi(patch_feats, patch_mask) -> Tensor"
        - "encode_rna(gene_ids, expr_values) -> Tensor"
        - "encode_dna(variant_vector) -> Tensor"
        - "contrastive_loss(wsi_emb, mol_emb) -> Tensor"
        - "training_step(batch) -> Dict[str, Tensor]"
      batch_requirements:
        - "Batch must carry patch_feats and patch_mask for WSI branch."
        - "Batch must carry either RNA inputs or DNA variant_vector per sample path."
        - "Contrastive objective must ignore unavailable modality paths in a batch-safe manner."

  feature_interface:
    conch_adapter:
      class_path: "src/models/encoders/conch_adapter.py"
      expected_feature_dim: 768
      input_preprocess_contract:
        resize_to: 448
        normalization: "ImageNet mean/std"
      note: "CONCH patch extraction is external to ThreadsModel; adapter enforces dimensional compatibility."

  training_integration:
    pretraining_module:
      class_path: "src/train/pretrain_module.py"
      uses:
        - "ThreadsModel"
        - "RankMe callback"
      required_model_fields:
        - "threads.output_dim"
        - "threads.slide_encoder.*"
        - "threads.rna_encoder.*"
        - "threads.dna_encoder.*"
        - "threads.multimodal_objective.temperature"
    finetuning_module:
      class_path: "src/train/finetune_module.py"
      uses:
        - "ThreadsSlideEncoder / ThreadsModel encode_wsi path"
      required_model_fields:
        - "threads.slide_encoder.*"
        - "threads.output_dim"

  invariants:
    - "Post-slide encoder embedding dimension is always 1024."
    - "RNA and DNA projection outputs must both be 1024 to share contrastive space with WSI embeddings."
    - "Slide encoder input dimension is fixed at 768 (CONCH feature dimension)."
    - "No architecture hyperparameters may be inferred beyond values present in config.yaml; unknowns remain null."

  unresolved_from_paper_and_config:
    - "Contrastive temperature is unspecified (null)."
    - "Some exact equation renderings are missing in parsed text; implementation must use standard gated-attention and InfoNCE forms while preserving listed interfaces."
    - "Exact scGPT checkpoint identifier/path is not provided and must be supplied externally at runtime."
