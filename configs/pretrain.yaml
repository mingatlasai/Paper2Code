# Pretraining configuration for THREADS reproduction.
# This file is consumed by:
# - src/pipelines/pretrain_pipeline.py
# - src/train/pretrain_trainer.py

pretrain:
  version: "1.0"

  stage_scope:
    mode: "pretrain"
    objective: "Cross-modal contrastive learning"

  environment:
    python_version: "3.10.12"
    pytorch_version: "2.3.0"
    cuda_version: "12.3"

  data:
    dataset_name: "MBTG-47K"
    total_wsi: 47171
    paired_rna_wsi: 26615
    paired_dna_wsi: 20556
    sources:
      mgh: 6899
      bwh: 20556
      tcga: 10209
      gtex: 9507

  preprocessing:
    tissue_segmentation:
      method: "FPN fine-tuned from segmentation_models_pytorch"
      checkpoint: null
    tiling:
      patch_size: 512
      stride: 512
      overlap: 0
      magnification: "20x"
      microns_per_pixel_approx: 0.5
      use_tissue_only: true
    patch_encoder:
      name: "CONCHV1.5"
      input_patch_size: 512
      resize_to: 448
      normalization: "ImageNet mean/std"
      output_dim: 768

  model:
    embedding_dim: 1024
    patch_encoder_output_dim: 768
    slide_encoder:
      type: "ABMIL (gated attention)"
      input_dim: 768
      hidden_dim: 1024
      output_dim: 1024
      num_heads_default: 2
      num_heads_ablations: [1, 2, 4, 6]
      pre_attention:
        hidden_layers: 3
        activation: "GELU"
        layer_norm: true
        dropout: 0.1
      attention:
        hidden_dim: 1024
        dropout: 0.25
        gated: true
    rna_encoder:
      base_model: "scGPT (pan-cancer checkpoint)"
      gene_identity_dim: 512
      expression_encoder: "2-layer MLP"
      transformer_layers: 12
      transformer_heads: 8
      projection_head_in_dim: 512
      projection_head_out_dim: 1024
      input_dropout: 0.2
      fine_tune_all_layers: true
    dna_encoder:
      input_dim: 1673
      architecture: "4-layer MLP (1 input, 2 hidden, 1 output)"
      hidden_dim: 1673
      output_dim: 1024
      activation: "ReLU"
      dropout: 0.2

  training:
    objective: "Cross-modal contrastive learning"
    optimizer:
      name: "AdamW"
      betas: null
      weight_decay: null
    hardware:
      gpus: 4
      gpu_type: "NVIDIA A100 80GB"
      distributed: "DDP"
      precision: "AMP (mixed precision)"
    schedule:
      batch_size_per_gpu: 300
      global_batch_size: 1200
      max_epochs: 101
      warmup_epochs: 5
      lr_warmup_start: 0.0
      lr_peak: 1.0e-5
      scheduler: "cosine decay after warmup"
    loss:
      family: "InfoNCE / CLIP-style contrastive"
      infonce_temperature: null

  checkpointing:
    criterion: "RankMe smooth rank increases"
    monitor_start: "after warmup"
    rankme:
      eps: 1.0e-7
    save_policy:
      save_on_rank_increase: true
      keep_best_only: false

  dataloader:
    drop_last_train: true
    drop_last_val: false
    pin_memory: true
    persistent_workers: true
    num_workers: 4

  runtime_defaults:
    seed: 42
    gradient_clip_norm: null
    detect_anomaly: false
    log_every_n_steps: 10
    validate_every_n_epochs: 1

  outputs:
    save_resolved_config: true
    save_epoch_metrics: true
    save_rankme_history: true
    save_best_checkpoint: true

  routing_rules:
    modality_values: ["rna", "dna"]
    use_all_patches_for_embedding: true
    patient_level_embedding_method: "union_of_all_patches_from_all_wsis"

  invariants:
    - "slide_embedding_dim == 1024"
    - "molecular_embedding_dim == 1024"
    - "patch_feature_dim == 768"
    - "global_batch_size == batch_size_per_gpu * gpus"
    - "RankMe monitoring starts after warmup"
    - "Unspecified hyperparameters remain null unless explicitly overridden"
