[{"role": "system", "content": "You are an expert researcher and strategic planner with a deep understanding of experimental design and reproducibility in scientific research. \nYou will receive a research paper in JSON format. \nYour task is to create a detailed and efficient plan to reproduce the experiments and methodologies described in the paper.\nThis plan should align precisely with the paper's methodology, experimental setup, and evaluation metrics. \n\nInstructions:\n\n1. Align with the Paper: Your plan must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.\n2. Be Clear and Structured: Present the plan in a well-organized and easy-to-follow format, breaking it down into actionable steps.\n3. Prioritize Efficiency: Optimize the plan for clarity and practical implementation while ensuring fidelity to the original experiments."}, {"role": "user", "content": "## Paper\n{'paper_id': '2025_MahmoodLab_A multimodal whole-slide foundation model for pathology', 'title': 'A multimodal whole-slide foundation model for pathology', 'abstract': '', 'pdf_parse': {'paper_id': '2025_MahmoodLab_A multimodal whole-slide foundation model for pathology', 'abstract': [], 'body_text': [{'text': 'The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning. However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose Transformer-based pathology Image and Text Alignment Network (TITAN), a multimodal whole-slide foundation model pretrained using 335,645 whole-slide images via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any fine-tuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that it outperforms both ROI and slide foundation models across machine learning settings, including linear probing, few-shot and zero-shot classification, rare cancer retrieval, cross-modal retrieval and pathology report generation.', 'section': '', 'sec_num': None}, {'text': \"Foundation models are transforming computational pathology by accelerating the development of AI tools for diagnosis, prognosis and biomarker prediction from digitized tissue sections 1 . Developed using self-supervised learning (SSL) on millions of histology image patches (or regions of interests), these models capture morphological patterns in histology patch embeddings, such as tissue organization and cellular structure [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] . These representations serve as a 'foundation' for models that predict clinical endpoints from whole-slide images (WSIs), such as diagnosis or biomarker status . However, translating the capabilities of current patch-based foundation models to address patient-and slide-level clinical challenges still remains complex due to the immense scale of gigapixel WSIs, compounded by the small size of patient cohorts in real-world evidence [39] [40] [41] [42] , especially for rare diseases with limited training data [43] [44] [45] . Similarly, given a diagnostically challenging WSI, retrieving a similar WSI via slide search 5, [46] [47] [48] [49] [50] [51] [52] [53] or pathology reports through cross-modal report search 10, [54] [55] [56] typically Article https://doi.org/10.1038/s41591-025-03982-3 levels of performance compared to existing slide foundation models, particularly in low-data regimes, language-guided zero-shot classification and rare cancer retrieval. Additionally, we demonstrate the utility of pretraining with synthetic fine-grained morphological descriptions, suggesting the scaling potential of TITAN pretraining with synthetic data [89] [90] [91] . Through comprehensive evaluation across a large range of clinical tasks, including the application to rare cancer retrieval, we demonstrate the efficacy of our vision-language pretraining approach, showcasing the general-purpose capability of our slide foundation model.\", 'section': '', 'sec_num': None}, {'text': 'Scaling SSL from histology patches to whole-slide images (WSIs)', 'section': 'Results', 'sec_num': None}, {'text': 'TITAN is a Vision Transformer (ViT) 92 that creates a general-purpose slide representation readily deployable in diverse clinical settings. It is pretrained on an internal dataset (termed Mass-340K) consisting of 335,645 WSIs and 182,862 medical reports (Fig. 1a ). To ensure the diversity of the pretraining dataset, which has proven to be a key factor in successful patch encoders 21 , Mass-340K is distributed across 20 organs, different stains, diverse tissue types and scanned with various scanner types (Fig. 1a and Supplementary Table 1 ). The pretraining strategy consists of three distinct stages to ensure that the resulting slide-level representations capture histomorphological semantics both at the ROI-level (4 \u00d7 4 mm 2 ) and at the WSI-level with the help of visual and language supervisory signals-stage 1, vision-only unimodal pretraining with Mass-340K on ROI crops (Fig. 1b, c ), stage 2, cross-modal alignment of generated morphological descriptions at ROI-level (423k pairs of 8k \u00d7 8k ROIs and captions) and stage 3, cross-modal alignment at WSI-level (183k pairs of WSIs and clinical reports; Fig. 1d ; see Methods for more details). For ease of notation, we refer to the model pretrained with vision-only in stage 1 as TITAN V and to the full model after all three stages of pretraining as TITAN.', 'section': 'Results', 'sec_num': None}, {'text': \"The cornerstone of our approach is emulating the patch encoder designed for input patch images at the slide level. Instead of using tokens from a partitioned image patch, the slide encoder takes a sequence of patch features encoded by powerful histology patch encoders 4, [7] [8] [9] [10] [11] [12] [13] [14] 58 . Consequently, all of TITAN pretraining stages occur in the embedding space based on pre-extracted patch features, with the patch encoder assuming the role of the 'patch embedding layer' in a conventional ViT (Fig. 1b ). To preserve the spatial context of each patch and consequently enable the use of positional encoding in the embedding space, the patch features are spatially arranged in a two-dimansional (2D) feature grid replicating the positions of the corresponding patches within the tissue (Fig. 1c ). Following the success of masked image modeling and knowledge distillation in patch encoders 21 , we apply the iBOT 86 framework for vision-only pretraining of TITAN on the 2D feature grid.\", 'section': 'Results', 'sec_num': None}, {'text': 'While the conceptual transition to slide-level is simple, this presents a new set of model design and pretraining challenges as follows:', 'section': 'Results', 'sec_num': None}, {'text': '(1) handling long and variable input sequences (>10 4 tokens at slide-level versus 196 to 256 tokens at the patch-level), (2) creating multiple views of one sample for SSL and (3) ambiguity over positional encoding schemes that capture local and global context in the tissue microenvironment. First, to tame the computational complexity caused by long input sequences, we construct the input embedding space by dividing each WSI into nonoverlapping patches of 512 \u00d7 512 pixels (instead of widely used 256 \u00d7 256 pixels) at \u00d720 magnification, followed by the extraction of 768-dimensional features for each patch with CONCHv1.5, the extended version of CONCH 10 . To address the issue of large and irregularly shaped WSIs, we create views of a WSI by randomly cropping the 2D feature grid (Fig. 1c ). Specifically, a region crop of 16 \u00d7 16 features covering a region of 8,192 \u00d7 8,192 pixels is randomly sampled from the WSI feature grid. From this region crop, two random global (14 \u00d7 14) and ten local (6 \u00d7 6) crops are sampled for iBOT pretraining. We further requires specialized algorithms to bridge the gap between fine-grained patch embeddings and slide-level information, introducing hurdles to clinical adoption.', 'section': 'Results', 'sec_num': None}, {'text': 'To overcome these limitations, new types of foundation models have recently been proposed for encoding entire WSIs into slide-level general-purpose feature representations [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] . Instead of training an additional model on top of patch embeddings from scratch 34, [73] [74] [75] [76] [77] [78] [79] [80] , these whole-slide representation models can be pretrained to distill pathology-specific knowledge from large WSI collections, simplifying clinical endpoint prediction with their off-the-shelf application. The outstanding challenge then becomes developing whole-slide foundation models that faithfully encode the tissue microenvironment based on a set of patch embeddings while also handling arbitrarily large WSIs. Although relatively underexplored, slide-level self-supervision can be performed with vision-only pretraining, either through masked image reconstruction 58 or intraslide contrastive learning 59, 60, 81 , or in a multimodal fashion involving pathology reports, bulk transcriptomics, or immunohistochemistry (IHC) [61] [62] [63] [64] 66, 67, 82 . Furthermore, long-range context modeling can either be neglected, essentially treating a WSI as a bag of independent features 59, [62] [63] [64] 72, 83 , or explicitly modeled using Transformers 57, 58, 60, 61 . With efforts to learn general-purpose slide representations intensifying, we believe that adapting successful patch-level recipes to the entire WSI would lead to powerful general-purpose slide representations.', 'section': 'Results', 'sec_num': None}, {'text': 'Despite their widespread application potential, previous works on pretraining slide foundation models have several shortcomings. First, these models are predominantly pretrained using vision-only modeling 57, 59, 60 , which neglects not only rich supervisory signals found in pathology reports but also precludes multimodal capabilities such as zero-shot visual-language understanding and cross-model retrieval-a fundamental hallmark in foundation models 84, 85 . Second, whereas current patch foundation models are trained with millions of histology image patches, slide foundation models are developed with orders of magnitude fewer samples and limited optimization of SSL recipes, leading to slide representations with restricted generalization capability 58, 62, 82, 83 . Even with multimodal techniques such as vision-language pretraining that augment the pretraining dataset with pathology reports, current slide foundation models still require end-to-end training or fine-tuning and lack the capability of learning transferable slide representations for challenging clinical scenarios 58, 82, 83 . Finally, the current models are nascent in transforming pathology AI model development due to their limited evaluations in diagnostically relevant settings, such as few-shot learning or slide retrieval.', 'section': 'Results', 'sec_num': None}, {'text': 'Here, we introduce Transformer-based pathology Image and Text Alignment Network (TITAN), a multimodal whole-slide vision-language model designed for general-purpose slide representation learning in histopathology. Building on the success of knowledge distillation and masked image modeling 86, 87 for patch encoder pretraining 21, 22 , TITAN introduces a large-scale pretraining paradigm that leverages millions of high-resolution region-of-interests (ROIs; at 8,192 \u00d7 8,192 pixels at 20\u00d7 magnification) for scalable WSI encoding. Trained using 336k WSIs across 20 organ types, vision-only TITAN produces general-purpose slide representations that can readily be applied to slide-level tasks such as cancer subtyping, biomarker prediction, outcome prognosis and slide retrieval tasks, outperforming supervised baselines and existing multimodal slide foundation models. To augment TITAN with language capabilities, we further fine-tune it by contrasting with 423k synthetic fine-grained ROI captions generated using PathChat 88 , a multimodal generative AI copilot for pathology and with 183k pathology reports at the slide level. By leveraging free-text morphological descriptions, TITAN gains the ability to generate pathology reports, perform zero-shot classification and enable cross-modal retrieval between histology slides and clinical reports. Pretraining TITAN on an extensive repository of multimodal pathology data unlocks higher ', 'section': 'Results', 'sec_num': None}, {'text': 'https://doi.org/10.1038/s41591-025-03982-3 augment these feature crops with vertical and horizontal flipping, followed by posterization feature augmentation 93 . Finally, to ensure that the limited context pretraining translates to slide-level tasks, we use attention with linear bias (ALiBi) for long-context extrapolation of TITAN at inference time 94 . Originally proposed for long-context inference in large language models, we extended ALiBi to 2D, where the linear bias is based on the relative Euclidean distance between features in the feature grid, which reflects the actual distances between patches in the tissue (Supplementary Tables 2 and 3 ; see Methods for more details).', 'section': 'Article', 'sec_num': None}, {'text': 'To equip our model with language capabilities, we implement two additional multimodal and multiscale pretraining strategies (stages 2 and 3) using a subset of Mass-340K (Fig. 1d ). These stages leverage language descriptions that exist at multiple morphological scales, from fine-grained descriptions in pathologist annotations or textbooks at the patch-or region-level (stage 2) to high-level descriptions in pathology reports at the slide-level (stage 3). For both stages, we use contrastive captioners (CoCa) 95 as the pretraining strategy that aligns ROI and slide representations with the corresponding captions and reports, while generating accurate descriptions at ROI-level or reports at slide level, respectively. The slide encoder (weights initialized with TITAN V ), the text encoder and the multimodal decoder are all finetuned as part of the pretraining. In stage 2, we pretrain TITAN V with 423,122 pairs of 8,192 \u00d7 8,192 pixels ROIs and synthetic captions generated by the vision-language copilot PathChat 88 . In stage 3, we further pretrain the model with 182,862 pairs of WSIs and corresponding pathology reports, resulting in our final model TITAN (see Methods for more details; Supplementary Tables 4 5 6 7 8 9 10 ).', 'section': 'Article', 'sec_num': None}, {'text': 'We evaluate TITAN, TITAN V and existing slide encoders on a large set of diverse slide-level tasks, including morphological subtyping and molecular classification by linear probing on the frozen slide embeddings. For tasks with multiple cohorts available, we perform cross-validation on one cohort, for example, from The Cancer Genome Atlas (TCGA) 96, 97 , and use the remaining cohorts, for example, from Clinical Proteomic Tumor Analysis Consortium (CPTAC) 98, 99 or Dartmouth-Hitchcock Medical Center (DHMC) 100, 101 , as an external test cohort. As baselines, we compare to recent publicly available slide foundation models, PRISM 62 , GigaPath 63 and CHIEF 83 . These models employ different slide-level pretraining strategies (PRISM, WSI-report contrastive pretraining; GigaPath, masked image reconstruction pretraining; CHIEF, supervised contrastive learning of cancerous versus noncancerous WSIs), different patch-level encoders (PRISM and GigaPath, 256 \u00d7 256 pixels at \u00d720 magnification; CHIEF, 256 \u00d7 256 pixels at \u00d710 magnification) and a varying number of WSIs for pretraining (PRISM, 1.7\u00d7; GigaPath, 0.49\u00d7; CHIEF, 0.18\u00d7 the WSIs used for TITAN pretraining). Except for CHIEF, the pretraining datasets of TITAN (Mass-340K), PRISM and GigaPath do not include TCGA and PANDA, which allows us to use these two datasets as benchmarking tasks without concern for data leakage 102 . Additionally, we compare our approach with mean pooling using the same CONCHv1.5 patch encoder as TITAN, a simple yet powerful unsupervised slide representation framework 65, 66, 103 .', 'section': 'TITAN improves region and slide-level diagnostic capabilities', 'sec_num': None}, {'text': 'Furthermore, for a comprehensive evaluation of the baselines, we introduce two tumor classification tasks based on the publicly available repository TCGA with the following two different context lengths:', 'section': 'TITAN improves region and slide-level diagnostic capabilities', 'sec_num': None}, {'text': '(1) main cancer type classification on ROIs (TCGA-Uniform-Tumor-8K or TCGA-UT-8K), a ROI-level cancer subtyping task with 32 classes, where we manually curated 25,495 tumor-containing regions of 8,192 \u00d7 8,192 pixels at \u00d720 magnification (~4 \u00d7 4 mm 2 ) across TCGA, covering the same tissue context as the region crops in TITAN V pretraining (Extended Data Fig. 1 and Supplementary Table 11 ) and ( 2) a slide-level pan-cancer classification (TCGA-OncoTree or TCGA-OT) task of OncoTree codes 104 with 46 classes, consisting of 11,186 formalin-fixed paraffin-embedded (FFPE) WSIs from TCGA (Supplementary Table 12 ; see Methods for more details).', 'section': 'TITAN improves region and slide-level diagnostic capabilities', 'sec_num': None}, {'text': 'We first assess how the pretraining data scale affects the downstream performance of TITAN V , focusing on the four subtyping tasks-TCGA-UT-8K, TCGA-OT, OT108 and EBRAINS. The purpose of these multiclass classification tasks is to assess the generalizability and richness of feature representations across diverse diagnostic classes. We observe that the performance increases on all four tasks as more pretraining data is used, where TITAN V with full Mass-340K exhibits an average increase of 3.65%, 3.21% and 1.21%, compared to 12.5%, 25% and 50%, respectively, of Mass-340K, where the same distribution across the organs was maintained (Fig. 2a and Supplementary Tables 13 14 15 16 ). Despite the difference in pretraining recipes, we observe the same general trend for the three other slide encoders, where PRISM outperforms GigaPath and CHIEF by 9.01% and 20.1% on average, having 3.4 times and 9.7 times the number of pretraining WSIs, respectively. Furthermore, we observe that TITAN and TITAN V , with 48.5 million and 42.1 million parameters, outperform heavier slide encoders PRISM and GigaPath, with 99.0 million and 86.3 million parameters, demonstrating superior parameter efficiency of our model (Fig. 2b ).', 'section': 'TITAN improves region and slide-level diagnostic capabilities', 'sec_num': None}, {'text': \"We next evaluate TITAN on a range of clinically relevant tasks that span morphological classification (14 tasks), grading (3 tasks), molecular classification (39 tasks) and survival prediction (6 tasks; Supplementary Tables 17 18 19 20 21 ). On average, we observe that TITAN and TITAN V outperform other slide encoders (Fig. 2c ), demonstrating the superior slide representation quality of our models. In particular, TITAN significantly outperforms all existing slide encoders in morphological subtyping tasks across the entire spectrum of diagnostic complexities, including fine-grade pan-cancer classification (challenging morphological classification tasks, as shown in Fig. 2c ) and noncancerous tasks, such as cardiac allograft assessment (cellular-mediated rejection) and renal allograft assessment (antibody and cellular-mediated rejection). TITAN and TITAN V achieve an average of +8.4% and 6.7%, respectively, in performance on multiclass (balanced accuracy) and binary subtyping tasks (area under the receiver operator curve (AUROC)) over the next best-performing model, PRISM (Fig. 2c and Supplementary Tables 22 23 24 25 26 27 28 29 30 31 32 33 ). In particular, TITAN V (and TITAN) not only outperforms others on TCGA-UT-8K with 8k \u00d7 8k context that the model was trained on (+6% and 7.5% over PRISM) but also on WSI-level tasks that involve the entire tissue context, where TITAN V benefits from the long-context extrapolation via ALiBi, for example, TCGA-OT (+7% and 9.5% over PRISM), OT108 (+10% and 16% over PRISM) and EBRAINS (+9% and 9.1% over PRISM). Even with other nonparametric evaluations with prototyping 105, 106 and 20 nearest-neighbor evaluation, which predicts each WSI's label based on the proximity to other WSI embeddings in the embedding space, we observe that TITAN and TITAN V maintain superior performance Supplementary Tables 22-33. On grading tasks, TITAN outperforms the next best models CHIEF on average by +3.2% and PRISM by +4% in quadratic-weighted Cohen's \u03ba, where the high performance of CHIEF can be attributed to including the dataset PANDA in pretraining (Supplementary Tables 34 35 36 ). To evaluate the molecular classification performance, we tested the model on tasks from public datasets (BCNB and MUT-HET) and internal-external paired public datasets (TCGA, CPTAC and EBRAINS), on IHC tasks, and MGB internal molecular tasks (Fig. 2c , Extended Data Fig. 2 and Supplementary Tables 37-63 ). Averaged across all molecular tasks, TITAN significantly outperforms its mean baseline on CONCHv1.5 features, GigaPath and CHIEF (P < 0.0001).\", 'section': 'TITAN improves region and slide-level diagnostic capabilities', 'sec_num': None}, {'text': 'On survival prediction tasks, we observe that TITAN and TITAN V are generally the best-performing baselines, outperforming the next best-performing model CHIEF by +3.62% and +2.90%, respectively, on concordance index for disease-specific survival 97 although CHIEF was pretrained on TCGA slides (Supplementary Table 64 ). Interestingly, the mean pooling baseline shows competitive performance, suggesting that the proportion of different morphological phenotypes is an important prognostic factor 65, 103 .', 'section': 'TITAN improves region and slide-level diagnostic capabilities', 'sec_num': None}, {'text': 'https://doi.org/10.1038/s41591-025-03982-3', 'section': 'Article', 'sec_num': None}, {'text': 'To further understand how the slide embedding space is organized and consequently affects the downstream performance, we visualize UMAP embeddings of WSIs from TCGA-OT colored by organ type, showing that TITAN and TITAN V form distinct organ clusters (for example, breast further separated from bladder, stomach and lung) better separated than with other slide encoders (Fig. 1e and ', 'section': 'Article', 'sec_num': None}, {'text': 'https://doi.org/10.1038/s41591-025-03982-3', 'section': 'Article', 'sec_num': None}, {'text': 'Extended Data Fig. 3 ). To investigate the robustness of TITAN to nonmorphology-related effects (that is, batch effects), we evaluate how well slide representations from TCGA-OT cluster according to tumor type, organ and TCGA submission site. Both TITAN and TITAN V mix submission sites well, while performing best in clustering them by biological factors, which suggests superior generalization capabilities (Extended Data Fig. 4 ). For interpretability analysis, we visualize attention heatmaps derived from the Transformer attention heads. The heatmaps indicate that different heads focus on distinct morphological regions such as dense tumor, tumor-adjacent stroma and nontumor regions, with majority of the heads focusing on dense tumor, consistently across multiple inputs (Extended Data Fig. 5 ).', 'section': 'Article', 'sec_num': None}, {'text': 'To assess the calibration and the confidence of predictions from TITAN, we implement the expected calibration error (ECE) 107 and entropy-based confidence score, and average both metrics across the four challenging subtyping tasks. Again, we observe that TITAN and TITAN V achieve the best calibration and the highest confidence prediction (Supplementary Table 65 ).', 'section': 'Article', 'sec_num': None}, {'text': 'Finally, to better understand how our model choices affect the downstream performance, we perform ablation experiments on the following four design choices of TITAN: the positional encoding, the number of transformer layers in TITAN V , the inclusion of vision pretraining and the region size in vision-only pretraining (Fig. 2d and Supplementary Tables 66 67 68 69 70 71 72 73 74 75 76 77 ; see Methods for more details). Averaged across the four challenging subtyping tasks, our results demonstrate that ALiBi positional encoding outperforms original absolute positional encoding 92 by +1.89%, six transformer layers provide best performance compared to 12 layers (+1.16%) and 4 layers (+1.74%), vision pretraining improves results by +2% over vision-language alignment alone, and a region size of 8,192 2 achieves the best balance between performance (+3.6% over smaller regions of 4,096 2 ) and computational efficiency.', 'section': 'Article', 'sec_num': None}, {'text': 'To further assess the quality of the slide embeddings and how application settings affect downstream performance, we evaluate different learning paradigms by comparing the linear probe performance of each slide encoder against other MIL models comprised of mean pooling, that is, averaging the patch embeddings, attention-based MIL (ABMIL) 73 and task-specific fine-tuning of the slide encoder from random or pretrained weights. For mean pooling and ABMIL, we use respective patch encoders for each slide encoder. This allows us to gauge whether the pretrained slide encoders have learned meaningful slide representations and outperform the simple yet powerful unsupervised (mean pooling) and supervised (ABMIL) baselines, neither of which involves large-scale pretraining.', 'section': 'Comparison with different learning paradigms for slide encoding', 'sec_num': None}, {'text': 'We observe several trends with TITAN (Fig. 2e , Extended Data Fig. 6 and Supplementary Tables 78 79 80 81 ). First, ABMIL outperforms mean pooling, as expected, since ABMIL is supervised and equivalent to weighted averaging of the patch features. Next, the linear probe outperforms ABMIL, demonstrating that multimodal self-supervised pretraining of TITAN and TITAN V effectively captures the contextual and semantic morphological details of the slide. This further suggests that our task-agnostic slide embeddings are better equipped for downstream tasks than task-specific supervised slide embeddings. Finally, we observe that task-specific fine-tuning of TITAN leads mostly to performance improvement over linear probe of TITAN and TITAN V while fine-tuning the slide encoder from randomly initialized weights yields lower performance (-3.63% on average). This suggests that the pretrained weights of TITAN V can serve as a good initialization for task-specific training, in line with previous works 62, 64 . One exception is OT108, which could be attributed to the small number of samples for each class (ranging from 4 to 42), which may lead to overfitting. However, we observe that other slide encoders do not necessarily follow such important trends, possibly suggesting suboptimal model pretraining and lack of generalizability.', 'section': 'Comparison with different learning paradigms for slide encoding', 'sec_num': None}, {'text': 'We also evaluate the data-constrained setting of few-shot learning, where only a few samples for each category are provided within the linear probe setting (Fig. 2f ; see Methods for more details). We observe that TITAN significantly outperforms all other encoders across different tasks and the number of shots (P < 0.0001), demonstrating strong generalizability. TITAN V is the second-best-performing model, again underscoring that vision-language alignment benefits the downstream task performance. Notably, TITAN and TITAN V exhibit especially high performance in one-shot learning, on par with other slide encoders trained on more shots (Supplementary Tables 82 83 84 85 ). Specifically, TITAN and TITAN V outperform CHIEF by 22.4% and 13.5% (TCGA-UT-8K) and 18.7% and 6.8% (TCGA-OT), respectively, on 16 shots, although CHIEF has been pretrained on TCGA slides.', 'section': 'Few-shot learning for low-data regime', 'sec_num': None}, {'text': 'Interestingly, both TITAN and TITAN V also outperform ABMIL with the same patch encoder across all settings, particularly in lower-shot settings. The largest gap for 1-shot is observed in the OT108 task, where TITAN outperforms ABMIL by 56.7%, with similar trends in prototyping evaluation (Supplementary Tables 86 87 88 89 ). Such superior data efficiency suggests that TITAN V can excel in rare cancer settings with a limited number of samples, such as OT108 in our benchmark, without the need for task-specific fine-tuning.', 'section': 'Few-shot learning for low-data regime', 'sec_num': None}, {'text': 'We further assess the language capabilities of TITAN by aligning the slide representations of TITAN V to language-based morphological descriptions. Specifically with TITAN, we assess the cross-modal zero-shot classification 55, 56, 108 and report-generation capabilities and study the effect of stage 2 pretraining for caption alignment with fine-grained morphological descriptions and stage 3 pretraining with coarse clinical reports of relevant microscopic findings.', 'section': 'Language-aligned TITAN enables cross-modal capabilities', 'sec_num': None}, {'text': 'To evaluate the quality of vision-language alignment, we first perform cross-modal zero-shot experimentation on 13 subtyping tasks of varying difficulties comparing with PRISM, also equipped with cross-modal capabilities (Fig. 3a ). In zero-shot classification, the diagnostic labels expressed as text prompts (Supplementary Tables 90 91 92 93 94 95 96 ) are encoded with the text encoder. The diagnostic prediction of the query slide is decided by the closest label embedding to the TITAN-encoded slide embedding, based on \u2113 2 distance in the embedding space. We observe that TITAN performs the best across these tasks, significantly outperforming PRISM by a large margin on multiclass classification tasks (balanced accuracy +56.52%) and binary subtyping tasks (AUROC +13.8%), for both cancer subtyping tasks and noncancerous tasks (Fig. 3b and Supplementary Tables 97 98 99 100 101 102 103 104 105 106 107 108 109 ). The performance gap between TITAN and PRISM is the widest on the 30-class EBRAINS subtyping task, where the balanced accuracy of TITAN is more than double that of PRISM (balanced accuracy of +121.9%).', 'section': 'Language-aligned TITAN enables cross-modal capabilities', 'sec_num': None}, {'text': 'To understand how different design considerations affect the zero-shot performance, we ablate over pretraining stages and the slide encoder architecture (Fig. 3c ). In total, we experiment with four variations of TITAN and present the average performance over four challenging subtyping tasks, TCGA-UT-8K, TCGA-OT, OT108 and EBRAINS (individual results can be found in Supplementary Tables 110 111 112 113 ). We observe that TITAN maintains the best overall zero-shot performance. Of the three pretraining stages, stage 1 vision pretraining contributes the least (balanced accuracy of -0.4% against TITAN), followed by stage 2 ROI caption alignment (-3.6% against TITAN) and stage 3 slide-report alignment (-7.3% against TITAN). This underscores the importance of aligning vision and language at both fine-grained and global levels, thereby combining the insights independently derived at patch-level 7, 10 and slide-level 58, 62, 72 , which is Article https://doi.org/10.1038/s41591-025-03982-3 lacking in report-only aligned baselines such as PRISM and GigaPath. Finally, a multiheaded ABMIL (MH-ABMIL) network, serving as the vision backbone with vision-language alignment pretraining, lags behind TITAN with and without vision pretraining by 1.94% and 1.54%, respectively. This indicates that the ViT architecture, incorporating self-attention and ALiBi, provides better downstream performance than attention-based alternatives.', 'section': 'Language-aligned TITAN enables cross-modal capabilities', 'sec_num': None}, {'text': \"Finally, we assess TITAN's capabilities of generating pathological reports, using the text decoder trained during CoCa pretraining. To this end, we introduce a report-generation task on TCGA, TCGA-Slide-Reports, consisting of 10,108 FFPE WSIs with paired slide-level reports parsed from 9,523 patient-level TCGA reports released by a previous study 109 (see Methods for more details). We evaluate the models using three metrics METEOR 110 , ROUGE 111 and BLEU 112 . We observe that TITAN outperforms PRISM by a large margin, on average by 161% across the three metrics (Fig. 3d ). Examples of the generated reports for TITAN considered high-quality by the pathologists are shown in Fig. 3e , often capable of correctly capturing key attributes such as tissue site, diagnosis and tumor grade as well as key representative morphology (Extended Data Fig. 7 ). \", 'section': 'Language-aligned TITAN enables cross-modal capabilities', 'sec_num': None}, {'text': 'The slide from the cervix uteri shows a poorly di erentiated squamous cell carcinoma.', 'section': 'Clinical report', 'sec_num': None}, {'text': 'Microscopic examination of the provided slide confirms a diagnosis of squamous cell carcinoma originating from the uterine cervix.', 'section': 'Ours (generated)', 'sec_num': None}, {'text': 'Diagnosis: high-grade Serous carcinoma of unspecified organ. ', 'section': 'PRISM (generated)', 'sec_num': None}, {'text': '2 mm T C G A -R C C ( C = 3 ) D H M C -R C C ( C = 3 ) D H M C -L U A D ( C = 5 ) C P T A C -N S C L C ( C = 2 )', 'section': 'PRISM (generated)', 'sec_num': None}, {'text': 'I m a g e Consulting cases with similar morphological features and diagnoses is essential for pathologists to make informed decisions, especially when dealing with complex or rare cases 5, 17, 47, 48, 50, 51, 53, 113, 114 . Retrieving similar histology slides or pathology reports facilitates the identification of relevant cases from large archival databases, and has become an essential clinical decision support in digital pathology workflows. This is especially beneficial for rare cancers that affect fewer than 15 individuals per 100,000 annually [43] [44] [45] , for which pathologists can identify nonspecific malignancies based on similar WSIs and their corresponding pathology reports. Slide foundation models readily provide WSI representations for vector database indexing, fundamentally simplifying the task of histology slide retrieval. Given a query slide and a set of support slides with diagnostic labels (indexed by a slide foundation model), histology slide search is evaluated by assessing the accuracy in retrieving identically labeled slides from the support set. Specifically, we test whether the K-closest neighbors of a query slide in the embedding space, determined using Euclidean distance with K = {1, 3, 5}, include slides of the same diagnostic label as the query (see Methods for more details).', 'section': 'PRISM (generated)', 'sec_num': None}, {'text': \"We design three variations of the rare cancer retrieval task, Rare-Cancer, Rare-Cancer-Public and Rare-Cancer-External, to assess generalization in different scenarios (see Methods for more details). For Rare-Cancer, we curate a large database of 186 cancer types with 19,626 WSIs by combining a 'rare cancer set' of 43 cancer types (3,039 WSIs) with the 'common cancer set' of 143 more common cancer types (16,587 WSIs) from TCGA, EBRAINS and MGB internal data (Fig. 4a and Supplementary Table 114 ). This emulates the real-world setting of clinicians interacting with an extensive cancer database encompassing a diverse mix of rare and common cancer types. A query set is the subset of the 'rare cancer set', ensuring representation of all 43 rare cancer types, and a support set contains all remaining WSIs of the 'rare cancer set and the common cancer set', ensuring representation of all 186 cancer types. For Rare-Cancer-Public, we curate a public version with 127 cancer types and 14,062 WSIs using the data from TCGA and EBRAINS, resulting in 29 rare cancer types (1,982 WSIs) and 98 common cancer types with lower diversity (12,080 WSIs; Supplementary Table 115 ). Finally, we curate Rare-Cancer-External for external validation, comprised of 39 WSIs covering 12 challenging rare ovary and soft tissue cancers from Kanagawa Cancer Center Hospital, Japan (Supplementary Table 116 ).\", 'section': 'PRISM (generated)', 'sec_num': None}, {'text': 'We observe that TITAN significantly outperforms other slide encoders on average with +14.8% in Accuracy@K and +18.1% in MVAcc@K to the next best model PRISM (Supplementary Table 117 ). On Rare-Cancer-External, we observe that our slide encoder is significantly more robust to the domain shift to the external institution than other slide encoders with +30.8% and +41.5% in Accuracy@K and +31.2% and +26.7% in MVAcc@K for TITAN and TITAN V to the next best model GigaPath (P < 0.0001; Supplementary Table 118 ). The trends in performance are preserved on Rare-Cancer-Public with slightly higher performance levels as the task is easier with a support set containing fewer cancer types (Supplementary Table 119 ). An example of rare cancer retrieval is demonstrated in Fig. 4b , where the closest slide to the paraganglioma query is also of paraganglioma with a high similarity of 0.794 and less similar slides are of different cancer types (haemangioma from brain, similarity of 0.341). One of the retrieved slides is pheochromocytoma with a high similarity of 0.651, agreeing with the clinical understanding that both are morphologically tightly connected as rare neuroendocrine tumors 115 (additional examples in Extended Data Fig. 8 ). With multiclass cancer subtyping tasks of varying difficulties, we also observe that both TITAN and TITAN V significantly outperform other slide encoders (P < 0.0001; Fig. 4c and Supplementary Tables 120 121 122 123 124 ).', 'section': 'PRISM (generated)', 'sec_num': None}, {'text': 'We further investigate the cross-modal retrieval performance of TITAN, as the slide and report embedding spaces are already aligned (see Methods for more details). We perform the cross-modal experiments on TCGA-Slide-Reports, our proposed dataset for report generation with 10,108 slide-report pairs (Supplementary Table 125 ). We observe that TITAN significantly outperforms PRISM on both retrieval tasks across all K retrievals, as measured with Recall@K for K = {1, 3, 5, 10}, with +10.5% and +20.5% on average for report-to-slide and slide-to-report retrieval tasks, respectively (Fig. 4d and Supplementary Tables 126 127 ). The strong performance of TITAN even with a single report (0.75) hints at the clinical potential. For a diagnostically challenging query slide, clinicians can benefit from sifting through retrieved past reports with similar diagnoses.', 'section': 'PRISM (generated)', 'sec_num': None}, {'text': 'We introduce TITAN, a multimodal whole-slide foundation model for pathology, which combines and elevates successful SSL recipes from the patch level to the slide level. Methodologically, TITAN employs histology knowledge distillation in the feature space (vision-only) and contrastive learning by aligning ROIs with synthetic captions and WSIs with reports (vision-language). Pretrained on 336k WSIs, TITAN, a ViT architecture equipped with ALiBi positional encoding for long-context extrapolation, produces powerful general-purpose slide representations for a large variety of downstream tasks even without task-specific fine-tuning. From cancer subtyping to molecular classification, TITAN consistently outperforms other state-of-the-art slide encoders, such as PRISM 62 , GigaPath 58 and CHIEF 83 . This superiority is maintained in data-constrained settings such as rare disease classification and histology slide retrieval, which underscores the representation quality of TITAN. Further aligning the vision-pretrained TITAN with 423k ROI-level captions generated by PathChat and 183k pathology reports equips the model with multimodal capabilities such as zero-shot diagnosis, slide-report retrieval and report generation. We observe that aligning the slide embedding with both the fine-grained (ROI captions) and coarse-level (pathology reports) descriptions is crucial for handling the multiscale information inherent in tissue slides.', 'section': 'Discussion', 'sec_num': None}, {'text': 'Detailed ablation analyses reveal further insights into TITAN. We observe that stage 1 unimodal pretraining of TITAN V captures morphological concepts already with much less data than existing slide encoders. In particular, TITAN V consistently outperforms its mean pooling and task-specific attention-based pooling baselines that use the same patch encoder as TITAN V , proving that unimodal pretraining effectively captures the context of patch features in contrast to existing unimodal slide encoders. Next, in addition to unlocking language-related capabilities, we observe that the vision-language alignment further enhances the representation quality of our vision-only model. Specifically, TITAN outperforms TITANV for slide-level tasks, with the strongest improvements observed in nonparametric evaluation settings. While slide embeddings from pretrained TITAN are already promising, especially in the low-data regime, task-specific fine-tuning of the pretrained model can further enhance the downstream performance for tasks with a large enough patient cohort, pointing to the flexibility of TITAN when applied to diverse clinical and data settings. We conjecture that some of these insights can be readily translated into other domains of pathology foundation models, such as hematopathology 116 , spatial transcriptomics 117 , 3D pathology 118 and multiplex imaging 119 .', 'section': 'Discussion', 'sec_num': None}, {'text': 'Providing multimodal slide embedding off-the-shelf presents immediate clinical potential to assist clinicians in their routine diagnostic workflows 85 . Presented with diagnostically challenging tissue slides, pathologists and oncologists can greatly benefit from being able to retrieve and analyze diagnostically similar slides or clinical reports 51 . This would lead to a reduction in patient misdiagnosis and interobserver variability. TITAN can accurately retrieve similar diagnostic slides and reports for challenging scenarios from a large number of cancer types (>100), as well as rare cancer types 45 where the corresponding slides have scarce representation in the database. That all of these tasks could be performed off-the-shelf with TITAN without a Article https://doi.org/10.1038/s41591-025-03982-3 dedicated algorithm for each underscores both the generalizability of TITAN slide embeddings and the simplicity of slide-level tasks with the pretrained slide encoders.', 'section': 'Discussion', 'sec_num': None}, {'text': 'Despite the encouraging performance of TITAN, our framework has a few shortcomings. First, pretraining on 8k \u00d7 8k region crops and extrapolating with ALiBi to the entire WSI may still not capture the full contextual information. Other positional encodings for extrapolation could address this limitation. Next, despite our best efforts to curate a diverse pretraining dataset, patch foundation models and, consequently, slide foundation models are susceptible to encoding nonbiological features, such as tissue processing sites and scanners, which may compromise their translational impact [120] [121] [122] [123] . We believe that systematic investigations similar to our robustness analysis and insights discovered 124, 125 , combined with ongoing efforts to curate larger and multi-institutional pretraining datasets can mitigate the issue. Next, clinical reports processing still poses a challenge for Renal-AMR (C=2) @1 @3 @5 Mean @1 @3 @5 Mean @1 @3 @5 Mean Report-to-slide retrieval Slide-to-report retrieval PRISM TITAN @1 @3 @5 Mean @ Number of retrieved slides @ Number of retrieved reports @10 @1 @3 @5 Mean @10 0.90 vision-language alignment. Incorporating comprehensive clinical information conducive to contrastive learning, while ensuring that it is linked to morphology to some degree, involves substantial manual tuning even with the automated processing pipelines. Restructuring the reports into distinctive morphology and molecular characteristics could facilitate more effective learning. Finally, Mass-340K contains fewer slides compared to other pretraining datasets used for patch encoders 12, 13, 126 and slide encoders 62, 72 . We believe that the already strong performance of TITAN, merged with efforts to expand Mass-340K, will further improve performance. Promisingly, TITAN can be scaled up in terms of data and architecture. WSIs and corresponding medical reports are routinely available and stored. The synthetic region-level captions can easily be generated with the generative AI model to provide a wealth of text guidance 88, 127 . Combining the additional data and a heavier architecture can potentially improve the performance, as demonstrated with patch encoders 12, 13, 126 . Additionally, improved patch representation quality is likely to enhance the quality of the downstream slide encoder.', 'section': 'Discussion', 'sec_num': None}, {'text': \"In conclusion, we envision TITAN and its future iterations being incorporated into practitioners' everyday toolkits for routine application and comparison with other task-specific supervised frameworks, together reaching higher levels of performance in clinically important tasks.\", 'section': 'Discussion', 'sec_num': None}, {'text': 'Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41591-025-03982-3.', 'section': 'Online content', 'sec_num': None}, {'text': 'https://doi.org/10.1038/s41591-025-03982-3', 'section': 'Online content', 'sec_num': None}, {'text': 'The retrospective analysis of internal pathology images and associated reports used in this study received approval from the Mass General Brigham Institutional Review Board. Before the computational analysis and model development, all internal digital data, including WSIs, pathology reports and electronic medical records, were anonymized. Since the study did not involve direct patient participation or recruitment, informed consent was waived for the analysis of archival pathology slides.', 'section': 'Ethics statement', 'sec_num': None}, {'text': 'For large-scale visual pretraining, we curated Mass-340K, a diverse dataset consisting of 335,645 WSIs across 20 organs, with 89.7% hematoxylin and eosin (H&E), 7.9% IHC, 2.3% special stains and 0.1% others, across different tissue types (neoplastic 70.0%, tissue damage response 8.4%, normal 4.7%, inflammatory 3.4% and others 13.5%), sourced from the combination of in-house histology slides at Mass General Brigham (MGB), consult slides sent to MGB and the GTEx consortium 128, 129 . Sourced from several sites, Mass-340K covers a wide range of tissue preprocessing protocols with diverse scanners and stainers.', 'section': 'Pretraining dataset', 'sec_num': None}, {'text': 'Scanner type. Setting aside the publicly available GTEx cohort for which the scanner type information is not available, we confirm that Mass-340K uses 16 different scanners from seven different manufacturers. Detailed data breakdown along scanner types can be found in Supplementary Table 1 .', 'section': 'Pretraining dataset', 'sec_num': None}, {'text': 'For the internally-curated cohort at MGB, the following stainers were used: Leica HistoCore Spectra (H&E), Agilent DAKO Cov-erStainer (H&E), Leica Bond III (molecular), Leica BOND PRIME (IHC), Agilent Dako AutoStainer Link 48 (IHC) and Agilent Dako Artisan Link Pro (special stain).', 'section': 'Stainer type.', 'sec_num': None}, {'text': \"Stains. The 27k IHC slides in Mass-340K span 100+ unique stains, without focus on particular biomarkers. The goal of IHC curation was to ensure that TITAN is exposed to a large set of slides with diverse tissue appearances during the pretraining process. For example, these stains include proliferation markers (Ki-67), lymphoid and hematopoietic markers (CD4, CD20) and oncogenes and tumor markers (MYC, BRAF, human epidermal growth factor receptor 2 (HER2)). In addition, Mass-340K contains 50+ unique special stains, such as Masson's trichrome and Congo red.\", 'section': 'Stainer type.', 'sec_num': None}, {'text': 'To explore the effects of data scale at the pretraining stage, we formed three additional partitions of Mass-340K, containing 12.5%, 25% and 50% of the original dataset. These partitions were sampled to maintain the ratio of different data sources and preserve organ distribution.', 'section': 'Stainer type.', 'sec_num': None}, {'text': 'For the initial stage of vision-language alignment (stage 2 of TITAN), we used synthetic captions generated by PathChat, a state-of-the-art multimodal large language model designed for pathology 88 . To go beyond the typically brief clinical reports focused on the final diagnosis, we prompted PathChat to generate detailed morphological descriptions of ROIs, providing important training data for models to capture complex pathological features. Using PathChat, we generated synthetic captions for 423,122 diverse ROIs of 8,192 \u00d7 8,192 pixels sampled from Mass-340K. Since PathChat cannot process inputs of size 8,192 \u00d7 8,192 pixels directly, we divide each ROI into 64 patches of size 1,024 \u00d7 1,024 pixels. To retain the most representative morphological features, we applied K-means clustering with K = 16 to the 64 patches and then randomly sampled one patch from each cluster. The resulting 16 morphologically representative 1,024 \u00d7 1,024 patches were subsequently fed to PathChat. To further enhance the diversity of these captions, we used Qwen2-7B-Instruct 130 to rewrite the generated captions, ensuring varied language structures and expressions. Detailed prompts for both PathChat and Qwen2, along with examples of generated and diversified captions, are provided in Supplementary Tables 4 and 5 .', 'section': 'Synthetic caption generation using PathChat.', 'sec_num': None}, {'text': 'For the second stage of visionlanguage alignment (stage 3 of TITAN), we curated a dataset of 182,862 slide-report pairs from a combination of in-house clinical reports and pathology notes from the GTEx consortium 129 . However, clinical reports are often noisy and are typically organized at the patient level, hence contain information on multiple slides from the same patient, complicating the slide-report alignment. To address this, we used a locally served Qwen2-7B-Instruct 130 model to extract slide-specific descriptions and remove sensitive information unrelated to pathological diagnosis, such as gross descriptions, hospital and doctor names and patient clinical history. Additionally, we applied the same rewriting strategy used for synthetic captions to diversify the report text. Example prompts used for report cleaning and rewriting can be found in Supplementary Tables 6 7 8 .', 'section': 'Curation of slide-report dataset.', 'sec_num': None}, {'text': 'Preprocessing. Similar to previous studies 9, 10, 74 , WSIs were preprocessed by tissue segmentation, tiling, and feature extraction using a pretrained patch encoder. We used the CLAM toolbox 74 for tissue segmentation and tiling. Tissues were segmented by binary thresholding of the saturation channel in HSV color space at a low resolution. Following this, we applied median blurring, morphological closing and filtering of contours below a minimum area to smooth tissue contours and eliminate artifacts. Nonoverlapping 512 \u00d7 512 pixel patches were then extracted from the segmented tissue regions of each WSI at \u00d720 magnification. For feature extraction, we used CONCHv1.5, an extended version of CONCH 10 , which was trained with 1.26 million image-caption pairs using the CoCa training objective for 20 epochs. The choice of CONCHv1.5 for feature extraction was due to the fact that the model was pretrained on histology regions with diverse stains and tissue types, including FFPE, frozen tissue and IHC, thereby yielding region features that are robust against diverse tissue processing protocols. By increasing the patch size from the widely used 256 \u00d7 256 pixels, we effectively reduce the sequence length by four without impacting the representation quality due to higher resolution patch input, leveraging the robustness of the patch-level foundation models in generalizing to higher resolutions 9, 10, 87 .', 'section': 'Unimodal visual pretraining', 'sec_num': None}, {'text': 'Refer to Supplementary Table 2 for detailed hyperparameters of the patch encoder.', 'section': 'Unimodal visual pretraining', 'sec_num': None}, {'text': 'To enhance the effectiveness of the ROI sampling strategy during stage 1 training of TITAN V , an additional preprocessing step was performed to group the segmented tissue contours based on their spatial proximity within the slide. This addresses the challenging cases where multiple tissue regions are interspersed with background areas, particularly for biopsy samples where tissue fragments are often widely dispersed and for samples with multiple slices placed on the same slide. Specifically, we grouped tissue contours into clusters based on their coordinates, resulting in tissue groups that contain densely packed tissue regions with minimal background regions between them. Furthermore, tissue groups that contained fewer than 16 patches were filtered out. This grouping operation produced a total of 345,782 tissue groups from Mass-340K.', 'section': 'Unimodal visual pretraining', 'sec_num': None}, {'text': 'For training TITAN V on Mass-340K, we use iBOT, a state-of-the-art SSL method that combines student-teacher knowledge distillation and masked image modeling 86 . As iBOT is applied in the patch embedding space, instead of the typical use case of the raw image space, we adapt the pretraining recipes as follows.', 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': 'View generation. During training, we create region crops randomly sampled from the tissue groups, each of which corresponds to a feature https://doi.org/10.1038/s41591-025-03982-3 grid of size 16 \u00d7 16, corresponding to a field of view of 8,192 \u00d7 8,192 pixels at \u00d720 magnification (Fig. 1b ). The random sampling of region crops, instead of precomputing fixed regions, increases the diversity of the training set and effectively acts as an additional data augmentation, as the model encounters different parts of the same WSI at each training epoch. A region crop contains 256 features, which is equivalent in length to training on images of 256 \u00d7 256 pixels with a token size of 16 \u00d7 16 in the typical natural image setting. From this region crop, two global views (14 \u00d7 14 crops) and ten local views (6 \u00d7 6 crops) are generated by cropping within the region crop without scaling or interpolation and subsequently fed to iBOT training. The 2D feature grid setup allows us to directly apply student-teacher knowledge distillation approaches, which typically require square crop inputs.', 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': 'To achieve realistic augmentations in the embedding space, existing methods have employed offline image augmentations in the pixel space 34, 59 by extracting multiple patch features from different views of a given patch. While effective, this approach limits the number of additional views and becomes computationally infeasible for large training datasets. Additionally, choosing color space augmentations tailored to histopathology that go beyond standard color transformations introduces additional computational overhead. A few recent approaches addressed the difficulty with training generative networks on the feature space to transform the features 131, 132 , but also introduced additional computational cost for training. Instead, we apply frozen feature augmentations, which have been shown to work well for a few-shot classification task in the feature space of pretrained ViTs 93 .', 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': \"Positional encoding. Traditional multiple instance learning methods consider the patches to be permutation-invariant within the slide. Despite the promising results, this approach ignores the tissue context, which can be essential for capturing the interaction in the tumor microenvironments and can thus affect the model's performance 133 . In this context, for TITAN, we employ positional encodings in the patch embedding space to break permutation invariance and encode tissue context. Furthermore, TITAN adopts the strategy of 'train short, test long' to ease the computational burden, which also requires positional information via positional encodings. Trained at the region crops (ROIs) of 8,192 \u00d7 8,192 pixels (train short), we directly apply TITAN on the whole slide during inference (test long). We used ALiBi, a method originally proposed for 1D sequence in large language models 94 . Absolute positional encoding, another popular alternative that works well for images at training sizes, was shown to have weak extrapolation abilities 94 . Unlike other positional encodings applied to the input features, ALiBi adds a bias to the query-key dot product during the computation of attention scores. ALiBi effectively penalizes the attention score for tokens that are further apart from each other. Formally, let q i \u2208 \u211d d and k j \u2208 \u211d d represent the i-th query and j-th key, respectively. The attention score, which is typically computed as softmax (q i k T j ), is modified with 1D ALiBi as softmax (q i k T j -m|i -j|), where m is a predefined slope specific to each attention head. Since the feature grids and the resulting views are of 2D grid structure, we extend ALiBi to 2D by incorporating the Euclidean distance between the patches i and j. The 2D ALiBi can be written as\", 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': 'EQUATION', 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': 'where i x , i y and j x , j y are the 2D grid coordinates of patches i and j. The x and y coordinates are defined as the 2D patch coordinates (at magnification \u00d720) divided by the patch size of 512.', 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': 'Network architecture and training details. For the slide encoder, we use a ViT 92 with six transformer layers, 12 attention heads of dimension 64, resulting in an embedding dimension of 768 and a hidden dimension of 3,072. This smaller architecture, compared to typical ViTs used in patch encoders, is chosen based on previous studies 57 , which suggest that a compact network suffices for slide representation learning in the embedding space, especially given the limited data scale of WSIs compared to histology patch datasets, which are on the scale of billions. The patch embedding layer is replaced by an MLP to process the feature inputs. We train the model for 270 epochs (equivalent to 91,260 iterations), distributed across four NVIDIA A100 80GB graphics processing units (GPUs) with a local batch size of 256 per GPU. For all training hyperparameters, refer to Supplementary Table 3 .', 'section': 'Pretraining protocol.', 'sec_num': None}, {'text': 'To enhance the unimodal capabilities of TITAN V , we further explored the multimodal vision-language alignment of TITAN V with clinical text. Training a multimodal foundation model, however, faces several limitations related to data and compute. First, paired slide-report data are scarce compared to the scale of millions of image-caption pairs for patches. Additionally, real-world clinical reports typically contain only brief diagnostic information, unlike the detailed morphological descriptions in educational captions for histology ROI images. Finally, contrastive learning-based cross-modal training typically requires a large batch size, which is computationally infeasible for WSIs.', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': \"To address these issues, we propose a two-stage continual pretraining approach (referred to as stage 2 and stage 3 for TITAN) that progressively aligns the model with increasing context. We first align synthetic captions for ROIs of 8,192 \u00d7 8,192 pixels, followed by real clinical reports for WSIs. With emphasis on detailed morphological descriptions, the first vision-language alignment stage allows the model to learn fine-grained pathological concepts using a large batch size. In the next stage, we further augment the model's understanding of diagnostic terminology and reasoning, targeted to enhance its zero-shot understanding in downstream tasks. The second stage also serves as a 'high-resolution fine-tuning' phase, adapting the model from the local contexts of ROIs to the full-scale global context of WSIs. Altogether, these two stages are designed to gradually build the model's ability to comprehend and generate meaningful vision-language representations for WSIs.\", 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'Network architecture and training details. Following the success of previous studies 10 , we use CoCa 95 , a state-of-the-art visual-language foundation model pretraining method, for both stages of vision-language alignment. The model consists of an image encoder, a text encoder and a multimodal text decoder. Using our unimodal TITAN V as the image backbone, we add two attentional pooler components on top. The first attentional pooler uses a single query (contrastive query) to pool a single global representation of the feature grids and enable cross-modal contrastive learning with text embeddings. This global WSI representation can then be used for zero-shot or unsupervised evaluation of TITAN on downstream tasks. The second attentional pooler uses n = 128 queries (reconstruction queries) to generate a set of 128 image tokens designed for interacting with the multimodal text decoder for caption generation. We use the pretrained text encoders and multimodal decoders of CONCHv1. 5 (ref. 10), each consisting of 12 transformer layers with an embedding dimension of 768 and a hidden dimension of 3,072.', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'For both stages, we used eight NVIDIA A100 80GB GPUs. During stage 2 vision-caption pretraining, we used a local batch size of 196 per GPU, with gradient accumulation of 2, resulting in an effective batch size of 3,136. For stage 3 vision-report pretraining, we randomly crop the WSIs to 64 \u00d7 64 feature grids, allowing for larger batch sizes while maintaining a large field of view, corresponding to 32,768 \u00d7 32,768 pixels, which already covers most slides in our pretraining dataset. We used a local batch size of 16 per GPU, with a gradient accumulation of 2 to achieve an effective batch size of 256. To avoid deteriorating the quality of the pretrained vision encoder, we used a smaller learning rate and weight decay, as well as a slow warm-up strategy for the vision Medicine backbone, following previous work 134 . For all hyperparameters, refer to Supplementary Tables 9 and 10 . setting Baselines. We compare TITAN (1) unsupervised baselines with four slide encoders, Prov-GigaPath (referred to as GigaPath) 58 , PRISM 62 , CHIEF 83 , and the mean pooling baselines with features from the respective patch encoders, (2) supervised baselines and our model TITAN against zero-shot baseline PRISM.', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': \"baselines. GigaPath uses LongNet architecture as the slide encoder, a ViT 92 the 'base configuration', replacing the vanilla dense attention with dilated attention. It was trained on 171,189\", 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'WSIs from Providence via masked autoencoder . As a patch encoder, GigaPath uses ViT-G/14 pretrained with DINOv2 (ref. on the same in-house dataset. While GigaPath further performed convision-language pretraining, we only assess the unimodal model, as the multimodal model is not publicly available. For performance analysis, we use the output of the Transformer layer 11 as slide representation, which yields the best results on downstream tasks and also agrees with the provided fine-tuning recipe. PRISM uses the Perceiver architecture as the slide encoder, incorporating CoCa-based and language alignment 95 195,344 specimen-report pairs, which comprise a total of 587,196 WSIs, each containing one or more WSIs. As for the patch encoder, PRISM uses Virchow , a ViT-H/14 pretrained with DINOv2 (ref. on an in-house dataset. CHIEF 83 applies attention-based feature aggregation, trained via slide-level contrastive learning and anatomic site information. The patch encoder is based on CTransPath 4 , a self-supervised SwinTransformer 137 trained on 15 million patches. In addition to the pretrained slide encoders, we evaluate mean pooling as a baseline, where the patch features are averaged within each slide, as it serves as a strong unsupervised baseline despite its [64] [65] [66] . While we mainly compare with mean pooling based on CONCHv1.5 patch features, we also provide results for mean pooling the corresponding patch encoders of each slide encoder for a subset of analyses. Supervised baselines. We compare TITAN against ABMIL and the of the pretrained slide encoders. For ABMIL, the model was with a batch size of 1 using the AdamW optimizer with weight 10 -5 and a Cosine annealing learning rate scheduler with peak learning rate 10 -4 over 20 epochs. The patch encoders were selected accordingly for each analysis. For GigaPath fine-tuning, we used the publicly available code, which uses a batch size of 1, AdamW optimizer with weight decay 0.05 and Cosine annealing learning rate scheduler with warm-up and base learning rate 2 \u00d7 10 over five epochs. For CHIEF fine-tuning, we also used the publicly available fine-tuning code. For tasks with a validation set, the best model is chosen based on the validation loss.', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'Cross-modal baselines. For cross-modal zero-shot retrieval and clinical report generation, we compare TITAN against PRISM .', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'Linear and k-nearest neighbor (k-NN) probe evaluation. To evaluate transfer capabilities and representation quality of slide encoders, we adopt recent work in representation learning with self-supervised frameworks and perform linear (logistic regression) and probing. For linear probing, we minimize cross-entropy loss using the scikit-learn L-BFGS solver with \u2113 2 regularization, selecting \u2113 2 from 45 logarithmically spaced values between 10 -6 and 10 based on the validation loss. The maximum number of L-BFGS iterations is set to 500. For datasets without a validation set, such as small datasets or few-shot we use the default values of \u2113 2 = 1 with 1,000 iterations. We additionally evaluated with k-NN probing, a nonparametrized measure to quantify the representation quality of fixed embeddings. We apply it in the following two settings: first, we follow SimpleShot to create a prototypical class representation by averaging all slide embeddings per diagnostic class 105 ; second, we use the scikit-learn implementation of k-NN with k = 20 following stability observations from SSL literature 87, 138 . In both settings, Euclidean distance is used as the dismetric based on the centered and normalized slide embeddings.', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': \"Slide retrieval. further evaluate the representation quality of different slide encoders, we perform content-based slide retrieval using slide-level classification datasets, where we retrieve slides with the same class label as a given query slide. Specifically, we extract slide features for all WSIs. The training and validation sets are combined to serve as the database of candidate slides (keys), and we treat each slide in the test set as a query slide. Before retrieval, we preprocess both keys and queries by centering the slide embeddings, which involves subtracting their Euclidean centroid, followed by 2 normalization. The similarity between the query and each candidate in the database is computed using the \u2113 2 distance metric, where a smaller distance a higher similarity. The retrieved slides are then sorted based on their similarities to the query. The class labels are used to evaluate the retrieval performance using Acc@K for K \u2208 {1, 3, 5}, which measures whether at least one of the top K retrieved slides shared the same class label as the query, and MVAcc@5, which considers the majority class label among the top five retrieved slides. Detailed descriptions of these metrics are provided in 'Evaluation metrics'.\", 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': \"Cross-modal retrieval. Leveraging the vision-language aligned embedding space, we also evaluate cross-modal retrieval performance TCGA-Slide-Reports. Specifically, we assess both slide-to-report and report-to-slide retrieval tasks. All slides and reports are embedded into a shared space using the vision and the text encoders, respectively, by \u2113 2 normalization. Retrieval is performed by calculating pairwise cosine similarity between the slide and report embeddings. Our class-based approach mirrors the unimodal slide retrieval, where retrieval is successful if the retrieved slide or report belongs to the same diagnostic class as the query. Performance is quantified using Recall@K for K \u2208 {1, 3, 5, 10} for the class-based approach, which measures the proportion of queries for which the correct result appears among the top K retrieved items. Additionally, we report the mean recall, computed as the average of the Recall@K values across the four levels. Further details on these metrics can be found in 'Evaluation metrics'.\", 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'Few-shot slide classification. We evaluate few-shot classification by the number of shots K in {1, 2, 4, 8, 16, 32}. For each we select K shots per class or all samples per class if the class has less than K samples. We follow previous studies that used the SimpleShot 105 framework for evaluation of the few-shot learning performance of self-supervised 9 SimpleShot computes a prototypical representation per by averaging all samples within that class. The distances to the class prototypes are then computed on the test set. All embeddings are centered and normalized based on the few-shot samples. To make the evaluation more comparable to supervised baselines, such as we also assess few-shot classification with linear probing. As no set is available in few-shot experiments, we use the default scikit-learn recipe with regularization strength \u2113 2 = 1 and up to 1,000 iterations of the L-BFGS solver. To mitigate sampling bias, we aggregate the results across 50 different runs, using random samples for training while keeping the test set fixed.', 'section': 'Vision-language continual pretraining', 'sec_num': None}, {'text': 'For survival analysis, we employed the linear Cox proportional hazards model on the disease-specific survival clinical endpoint. We note that this differs from typical MIL survival prediction with negative log likelihood , as we deal with a single embedding for the slide (as opposed to a bag of patch embeddings), and patients can be batched (as opposed to the single patient per https://doi.org/10.1038/s41591-025-03982-3 batch due to memory usage). To reduce the impact of batch effects, we performed a five-fold site-preserved stratification 140 . Due to the small cohort size reliable survival prediction modeling, we used four folds training and the remaining for evaluation, without employing the validation fold. A hyperparameter \u03b1 was searched over 25 logarithmically values between 10 1 and 10 5 , with the \u2113 2 defined as C = \u03b1. For each combination of encoder and cancer type, we that yielded the best average test metric across the five folds. For fitting and testing the Cox model, we used the scikit-surv package.', 'section': 'Survival analysis.', 'sec_num': None}, {'text': 'Zero-shot slide classification. For zero-shot slide classification, we adopted the method described in CLIP 108 to use the similarities between given slide and the text prompts of each class as its prediction logits. Specifically, for a class c \u2208 {1, 2, \u2026, C}, we first created the text prompts for each class, followed by extracting their \u2113 2 -normalized text embeddings v c using the text encoder. Since the model could be sensitive to the specific choice of text prompts, we created an ensemble of prompts for each class. The complete set of prompt ensembles are provided in Supplementary Table 103 . For each WSI, we similarly computed a \u2113 2 -normalized embedding u i using the slide encoder. We then calculated the cosine similarity between the slide embedding and each class text embedding. The predicted class for a slide was the one with the highest similarity score:', 'section': 'Survival analysis.', 'sec_num': None}, {'text': '\u0177i = argmax c u i T v c (2)', 'section': 'Survival analysis.', 'sec_num': None}, {'text': \"generation. Slide captioning provides concise and interpretable summaries of visual findings in pathology, potentially enhancing clinical workflows. The generative objective of CoCa enabled the modcapabilities of generating pathological reports, which we explored 10,108 slide-report pairs from TCGA. We performed zero-shot captioning using TITAN and compared the quality of the generated report against PRISM . Specifically, we use a beam search decoding strategy 5 beams and 1 beam group, where the model explores five potential sequences at each step and retains only the most likely sequence within a single group to maximize quality while minimizing redundancy. metrics. We report balanced accuracy and weighted F1-score for all classification tasks with more than two classes. For ordinal multiclass classification tasks, we report balanced accuracy and quadratic-weighted Cohen's \u03ba. For binary classification tasks, we balanced accuracy and AUROC. For survival tasks, we report the concordance index (c-index), which measures the agreement between the model's predicted risks and the actual survival times. The expected calibration error (ECE) 107 measures whether the model's predicted match the actual frequencies of each diagnostic label, with the lower value indicating that the model's confidence estimates are well-calibrated. We use a multiclass variant of the original ECE, with one-versus-all binarization of the labels with respect to a given diagnostic label computed and averaged across all labels. The entropy score measures the uncertainty of predictions, with a lower value indicating that the model has higher confidence in its predictions. The entropy of the predicted probabilities was computed.\", 'section': 'Survival analysis.', 'sec_num': None}, {'text': 'For slide retrieval tasks, we report Acc@K for K \u2208 {1, 3, 5}, which measures if at least one slide among the top retrieved slides has the same class label as the query. We also report MVAcc@5, which is a stricter metric that considers whether the majority vote of the top 5 retrieved slides is in the same class as the query. For cross-modal retrieval tasks, we report Recall@K for K \u2208 {1, 3, 5, 10}, which measures the proportion of queries for which the correct result appears in the top K retrieved items. We also report mean recall, which is calculated as the average of the four Recall@K values. For report generation, we compare the generated reports with the ground truth pathological reports using METEOR, ROUGE and BLEU. METEOR 110 is a metric that evaluates text quality through unigram matching by considering both precision and recall while also accounting for synonyms, stemming and word order between the candidate and reference texts. ROUGE 111 comthe overlap of n-gram, word sequences and word pairs between the generated and reference texts, focusing on recall. We use ROUGE-1, which specifically measures the overlap of unigrams. BLEU 112 ', 'section': 'Survival analysis.', 'sec_num': None}, {'text': 'We use BLEU-1, which evaluates the extent of word-level between the generated and reference texts.', 'section': 'measures the quality of generated text based on unigram overlap, focusing on', 'sec_num': None}, {'text': 'For the datasets with five-fold splits, where we employ five-fold cross-validation, we report the mean performance the s.d. across all folds. For the datasets with a single split, we use nonparametric bootstrapping with 1,000 samples to calculate the mean and s.d.', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': 'To compare the performance of multiple methods across different datasets, we used a hierarchical generalized linear mixed-effects model (GLMM). A GLMM is a statistical model that enables analysis of the data both fixed and random effects. Specifically, we are interested in estimating the effect of each method (fixed effects) while accounting for variability across datasets (random effects). The hierarchical structure the fact that datasets differ in their overall performance levels, while the mixed-effects framework ensures that method comparisons are made after adjusting for these dataset-specific effects. Since the metric is bounded between 0 and 1, we used a distribution, parameterized in terms of a mean ij and a precision parameter \u03d5. The expected value of the metric for method j on dataset i is modeled as:', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': 'y ij \u223c \u03b2 (\u03bc ij , \u03d5), logit (\u03bc ij ) = \u03b1 + \u03b2 j + b i ,', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': 'the mean \u03bc ij was linked to the predictors using a logit with', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': '\u2022 \u03b1 is the overall intercept, \u03b2 j is the fixed effect of method \u2022 b i is a random intercept for dataset modeled with Gaussian distribution, that is, b i ~ n(0, \u03c3 2 ).', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': 'This approach accounts for the possibility that some datasets may consistently produce higher or lower performance scores, preventing these systematic differences from being misattributed to the methods themselves. We assume that, while absolute performance scores vary across datasets, the relative ranking of methods remains approximately consistent (for example, if Method A tends to outperform Method B, it is likely to do so across most datasets). Parameters were fitted using the likelihood estimation, and model fit was assessed through diagnostic checks of residual distributions and variance components.', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': 'compare methods, we compute estimated marginal means-the predicted average performance for each method adjusted for dataset-level variability. Pairwise comparisons of these means are conducted using two-sided Wald z tests, with the Tukey correction applied to control for comparisons and ensure robust inference. We also evaluate few-shot learning performance, where methods are compared with limited training examples (K = 1, 2, 4, 8, 16). For a given task (or dataset), to isolate the effect of method choice, we include the number of training examples as the random effect. We use a hierarchical GLMM with a \u03b2 distribution and compute estimated marginal means, with correction for multiple hypothesis testing, to assess substantial performance differences exist between models. For the retrieval tasks, we follow a similar approach to the few-shot by treating different numbers of retrieved samples as the random effect.', 'section': 'Statistical analysis.', 'sec_num': None}, {'text': 'For the evaluation of TITAN on a diverse set of downstream tasks (Supplementary Tables 18 19 20 21 ), we re-arrange the pre-extracted features from patches of 512 \u00d7 512 pixels to feature grids https://doi.org/10.1038/s41591-025-03982-3', 'section': 'evaluation datasets', 'sec_num': None}, {'text': 'cropped the tissue regions of the WSIs. Additionally, background masks are created to mask out features corresponding to background patches. Each WSI is then one single input to TITAN. For downstream tasks with patient-level annotations, we create patient embeddings by averaging all slide embeddings of TITAN corresponding to a single patient. In the following, we detail all sets used in our downstream evaluations, including splits and targets. We first describe the six datasets that we introduce in our study, TCGA-OncoTree, TCGA-Slide-Reports, Rare-Cancer, Rare-Cancer-Public and Rare-Cancer-External, followed by existing datasets in alphabetical order. To mitigate the impact of effects, all datasets based on TCGA are split into label-stratified and site-preserving folds such that slides from one clinical site only occur in one fold following 140 .', 'section': 'evaluation datasets', 'sec_num': None}, {'text': '). The TCGA-UT-8K dataset is a region-level pan-cancer subtyping resource comprising 25,495 of 8,192 \u00d7 8,192 pixels. These regions were extracted from 9,662 H&E-stained FFPE diagnostic histopathology WSIs sourced from TCGA. The tumor regions were manually annotated by two expert pathologists, with slide exclusion due to poor staining, poor focus, lacking cancerous regions and incorrect cancer types. Approximately three representative tumor regions per WSI were annotated with pixel-level contours. For each contour, we center-cropped an image region of pixels to encompass both the dense tumor and its surtissue context. We split the regions into train-validation-test (train-val-test; 13,853:3,434:8,208 slides), preserving the source site. Refer to Supplementary Table 11 for a detailed overview of all classes contained in this dataset.', 'section': 'TCGA-UniformTumor-8K (TCGA-UT-8K', 'sec_num': None}, {'text': 'TCGA-OncoTree (TCGA-OT). TCGA-OT is a pan-cancer subtyping dataset of 11,186 H&E FFPE diagnostic histopathology WSIs from TCGA 96 All WSIs are classified into 46 classes according to the OncoTree system, such that every class is represented by at least 50 We select all diagnostic H&E FFPE WSIs from TCGA with primary tumors. Concretely, we exclude frozen tissue slides, slides without magnification information, metastatic or recurrent tumor slides, slides tumor tissue and IHC slides. For training and evaluation, we the dataset into training-validation-test folds of 8,226:1,612:1,348 samples while preserving the source sites; that is, all slides from one source site are in one split. Refer to Supplementary Table 12 for a detailed overview of all classes.', 'section': 'TCGA-UniformTumor-8K (TCGA-UT-8K', 'sec_num': None}, {'text': \"TCGA-Slide-Reports. The TCGA-Slide-Reports is a pan-cancer slide-report dataset of H&E FFPE diagnostic histopathology WSIs from TCGA 96 . The dataset consists of 10,108 WSIs with paired pathological reports at the slide level. The dataset is built on the TCGA-Reports which consists of 9,523 patient-level reports released by a previous study 109 . The dataset TCGA-Reports was created using pathology report PDFs, corresponding to 11,010 patients, availon the TCGA data portal. The raw reports were preprocessed by removing 82 patients with multiple reports, 399 patients with tumors, 72 patients with no survival data, 381 'missing reports and 212 'TCGA Pathologic Diagnosis Discrepancy reports, resulting in 9,850 reports. Optical character recognition was then performed to extract text from the PDFs, followed by the removal of 'Consolidated Diagnostic Pathology Form' reports, 'Synoptic Translated' forms, within-report TCGA metadata insertions and clinically irrelevant reports, resulting in 9,523 patient-level reports. While these reports are clean and clinically relevant, they often contain descriptions of multiple tissue blocks per patient. This lack of one-to-one mapping between slides and reports poses a challenge for slide-level report generation and cross-modal retrieval, which require distinct slide-to-report alignment. Since block IDs are unavailable in TCGA metadata, we used the slide-level diagnoses to map diagnoses in each tissue block description. Specifically, if a block's diagnosis matched the slide-level diagnosis, we designated it as corresponding to the slide. This process was automated using resulting in a final set of 10,108 slide-report pairs. These paired slides are all H&E FFPE WSIs from primary tumors adhering to the same exclusion criteria as mentioned for TCGA-OT. We excluded all frozen tissue slides, slides without magnification information, metastatic or recurrent tumor slides, slides without tumor tissue and IHC slides. Refer to Supplementary Table 125 24 . Each biopsy is labeled for the presence of cardiac rejection, characterized by acute cellular rejection (no rejection, 866 patients; rejection, https://doi.org/10.1038/s41591-025-03982-3 822 patients). We split the dataset label-stratified on the patient level into train, val and test splits by a ratio of 70:10:20 (3547:484:990 slides).\", 'section': 'TCGA-UniformTumor-8K (TCGA-UT-8K', 'sec_num': None}, {'text': 'The DHMC-LUAD consists of 143 H&E FFPE WSIs of lung adenocarcinoma (LUAD) from the Department of Pathology and Laboratory Medicine at DHMC 100 . All WSIs are labeled into five classes of the predominant patterns of LUAD (acinar, 59; lepidic, 19; micropapillary, 9; papillary, 5; solid, 51). Given the limited size of the dataset, we use it exclusively for evaluation in a zero-shot setting, where we use the entire dataset as test set.', 'section': 'DHMC-LUAD.', 'sec_num': None}, {'text': 'The DHMC-RCC consists of 563 H&E FFPE WSIs of renal cell carcinoma (RCC) from DHMC 101 . All slides are labeled into the four predominant patterns of RCC, including one benign class (renal oncocytoma, chromophobe RCC, clear cell RCC, papillary RCC). We use the three RCC subtypes as an external test set for the three-class subtyping task, TCGA RCC.', 'section': 'DHMC-RCC.', 'sec_num': None}, {'text': 'The EBRAINS consists of 2,319 H&E FFPE diagnostic histopathology WSIs from the EBRAINS Digital Tumor Atlas sourced from the University of Vienna 143 . Due to the small sample size, we exclude two classes and predict a fine-grained 30-class brain tumor subtyping All brain tumors in these tasks are designated as rare cancers the project and the NCI-SEER program. For training and evaluation, we approximately label-stratified the dataset into a train-val-test fold with a ratio of 50:25:25 (1,151:595:573 slides). Additionally, we use 873 samples with annotations for isocitrate dehydrogenase 1 (IDH1) mutation as an external test set for IDH1 mutation prediction on the TCGA-Glioblastoma Multiforme and Lower-Grade Glioma (GBMLGG) cohort.', 'section': 'EBRAINS.', 'sec_num': None}, {'text': 'The IMP-CRC consists of 5,333 H&E FFPE colorectal biopsy and polypectomy WSIs retrieved from the data archive of IMP Diagnostics laboratory, Portugal [144] [145] [146] . All cases are classified into one of the following three categories: non-neoplastic (847 slides), low-grade lesions (2,847 slides), which include conventional adenomas with low-grade dysplasia, and high-grade lesions (1,639 slides), which include conventional adenomas with high-grade dysplasia, intramucosal carcinomas and invasive adenocarcinomas. We split the dataset label-stratified by a ratio of 60:20:20 into train-val-test set slides).', 'section': 'IMP-CRC.', 'sec_num': None}, {'text': 'MGB-BRCA. MGB-BRCA consists of 1,264 H&E FFPE WSIs of biopsies and resections of invasive breast cancers (BRCA) from BWH . Each case is annotated with the following three IHC status prediction tasks: ER status prediction (negative, 261; positive, 613), PR status prediction (negative, 37; positive, 504) and HER2 status prediction (negative, 665; positive, 151), where ER, PR and HER2 status were manuextracted from pathology reports.', 'section': 'IMP-CRC.', 'sec_num': None}, {'text': 'The MGB-LUAD consists of 1,939 H&E FFPE WSIs of LUAD from BWH 66, 124 . The WSIs are annotated by five molecular tasks with ground truth from IHC-protein 40 (P40) status prediction (negative, 113; positive, 72), protein 63 (P63) status prediction (negative, 72; positive, 81), Napsin A status prediction (negative, 60; positive, 66), caudal type homeobox 2 (CDX2) status prediction (negative, 55; posi-24) and cytokeratin 5 and 6 (CK-5&6) status prediction (negative, 29; positive, 29).', 'section': 'MGB-LUAD.', 'sec_num': None}, {'text': 'The MGH-BRCA consists of 1,071 IHC FFPE WSIs of invasive breast carcinoma from Mass Hospital 66 . The cases contain annotations for IHC quantification in six expression levels of ER abundance (levels 1-6 with counts-168, 169, 219, 170, 175 and 169, respectively) and PR abundance (levels 1-6 with counts-2,603, 2,397, 1,209, 1,118, 1,124 and 1,101, respectively).', 'section': 'MGH-BRCA.', 'sec_num': None}, {'text': 'The MUT-HET consists of 1,291 H&E FFPE WSIs of clear cell RCC, each representing a single patient treated at the Mayo Clinic 147, 148 . All cases are labeled with the following mutations, determined from IHC slides-BAP1 mutation (WT, 1,130; MUT, 162), PBRM1 mutation (WT, 622; MUT, 670) and SETD2 mutation (WT, 943; MUT, 349). We split the dataset into five splits train-val-test ratio of 60:20:20 (774:258:259 slides) in each split.', 'section': 'MUT-HET.', 'sec_num': None}, {'text': 'OT108. The OT108 is an in-house pan-cancer subtyping dataset consisting of 5,564 H&E FFPE diagnostic WSIs from classified into 108 according to the OncoTree classification 104 . We split the into train-val-test (3,164: The PD-L1 consists of 234 IHC FFPE diagnostic histopathology WSIs from 217 patients with stage IV nonsmall cell lung cancer (NSCLC) who initiated treatment with anti-PD-(L)1 blockade therapy between and 2019 at Memorial Sloan Kettering Cancer Center 150 . Patients who received chemotherapy concurrently with immunotherapy were included. We used the clinical PD-L1 assessments as labels and these labels by pathologist re-annotations on 157 slides when available. Following the original study, we created three levels of PD-L1 expression (<1%, 62; 1-50%, 49; \u226550%, 123) as target predictions.', 'section': 'MUT-HET.', 'sec_num': None}, {'text': 'split the dataset into five splits with train-val-test ratio of 60:20:20 (129:44:44 slides) in each split.', 'section': 'MUT-HET.', 'sec_num': None}, {'text': 'H&E FFPE WSIs of renal allograft biopsies from 1,118 patients collected at BWH between 2013 and 2022. Each case has associated labels for antibody-mediated rejection (AMR) status (AMR, 286 patients; no AMR, 832 patients), cellular-mediated rejection (cellular rejection, 341; no cellular rejection, 777) and interstitial fibrosis and tubular atrophy (IFTA) status (advanced IFTA, 162 patients; mild IFTA, 706 patients; moderate IFTA, 250 patients). We split the dataset into a label-stratified set (3002:376:824 slides). TCGA-CRC. The TCGA-CRC consists of 549 H&E FFPE diagnostic his-WSIs from TCGA of 543 patients with colorectal cancer (CRC). We predict microsatellite instability (61 patients) and microsatellite stable (353 patients), mutations in the genes BRAF (WT, 429 patients; MUT, 58 patients) and KRAS (WT, 286 patients; MUT, 201 patients), and tumor staging (T1, 16 slides; T2, 97 slides; T3, 372 slides; T4, 64 slides). CPTAC-COAD with 107 H&E FFPE diagnostic histopa-WSIs from 103 patients with colon adenocarcinoma serves as external validation dataset for all tasks (microsatellite instability, 24 patients; microsatellite stable, 79 patients; BRAF WT, 16 patients; MUT, 87 patients; KRAS WT, 36 patients; KRAS MUT, 58 patients; T2, 17 slides; T2, 77 slides; T4, 13 slides).', 'section': 'Renal allograft rejection. renal allograft rejection consists of', 'sec_num': None}, {'text': 'The TCGA-GBMLGG consists of 1,123 H&E FFPE diagnostic histopathology WSIs from TCGA of 558 patients with gliomas, more specifically GBMLGG. The WSIs are classified into the following two classes: IDH1 mutation (425 slides) and no IDH1 mutation (698 slides). EBRAINS serves as an external cohort for this task (IDH1 MUT, 333 slides; IDH1 WT, 540 slides).', 'section': 'TCGA-GBMLGG.', 'sec_num': None}, {'text': 'We used Python (version 3.9.16) for all experiments and analyses in the study, which can be replicated using open-source libraries as outlined below. We used PyTorch (version 2.0.1, CUDA 11.8) for training and inference of our deep learning model. To train TITAN V and TITAN, we modified the public implementation of iBOT (http://github.com/bytedance/ibot) and CoCa (http://github.com/mlfoundations/open_clip). We used 4\u00d7 and 8\u00d7 80GB NVIDIA A100 GPUs configured for multi-GPU training using distributed data parallelism for TITAN V and TITAN training, respectively. All downstream experiments were conducted on a single 24GB NVIDIA 3090 GPUs. All WSI processing was supported by OpenSlide (version 4.3.1), openslide-python (version 1.2.0) and CLAM (http://github.com/mahmoodlab/CLAM). We used Scikit-learn (version 1.2.2) for its implementation of k-NN, and the logistic regression implementation and SimpleShot implementation provided by the LGSSL codebase (http://github.com/mbanani/lgssl). For survival tasks, we used scikit-survival (Version 0.23.1). Implementations of other encoders benchmarked in the study are found at the following links: GigaPath (http://github.com/prov-gigapath/prov-gigapath), PRISM (https://huggingface.co/paige-ai/Prism) and CHIEF (http:// github.com/hms-dbmi/CHIEF). For training weakly-supervised ABMIL models, we adapted the training scaffold code from the CLAM codebase (http://github.com/mahmoodlab/CLAM). Matplotlib (ver-3.8.4) and Seaborn (version 0.13.2) were used to create plots in Figs. 1 2 3 4 . of other miscellaneous Python libraries is listed in the Reporting summary. Change in balanced accuracy performance for several learning paradigms on four subtyping tasks with respect to the linear probe. The baselines include mean pooling, ABMIL, linear probe, and finetuned from pretrained or randomly initialized weights. The number under each task name indicates the linear probe performance. TITAN-L represents the variation of TITAN without visionpretraining. For mean pooling and ABMIL, we use the respective patch encoder for each framework, as specified under each slide encoder name. Finetuning are not provided for PRISM, as the finetuning recipes were not available.', 'section': 'Computing software and hardware', 'sec_num': None}], 'back_matter': [{'text': \"This study was funded in part by the BWH President's Fund, MGH Pathology and by the National Institute of Health (NIH) National Institute of General Medical Sciences (NIGMS; R35GM138216 to F.M.). S.J.W. was supported by the Helmholtz Association under the joint research school 'Munich School for Data Science-MUDS' and the Add-on Fellowship of the Joachim Herz Foundation. This work was supported by a fellowship of the German Academic Exchange Service (DAAD). M.Y.L. was supported by the Tau Beta Pi Fellowship and the Siebel Foundation. This work was additionally supported by the AMED Practical Research for Innovative Cancer Control (grant JP 25ck0106873 to S.I.). We thank K. Ono at the Department of Pathology, Kanagawa Cancer Center Hospital, Kanagawa, Japan, for her contribution to the case collection of the Rare Cancer External dataset. The content is solely the responsibility of the authors and does not reflect the official views of the NIH, NIGMS, NCI and DoD.\", 'section': 'Acknowledgements', 'sec_num': None}, {'text': 'GTEx data used in pretraining can be accessed through the GTEx portal (https://www.gtexportal.org/home/). For benchmarks, TCGA and CPTAC data can be accessed through the NIH genomic data commons (https://portal.gdc.cancer.gov) and proteomics data commons (https://proteomic.datacommons.cancer.gov), respectively. Coordinates and labels of TCGA-UniformTumor-8K dataset is made publicly available in the TITAN GitHub repository. All other publicly available datasets benchmarked in this work can be can accessed in their respective data portals: EBRAINS (https:// DHMC RCC (https://bmirds.github. io/KidneyCancer), DHMC LUAD (https://bmirds.github.io/Lung-Cancer/), BRACS (https://bracs.icar.cnr.it), PANDA (https://panda. grand-challenge.org), IMP (https://rdm.inesctec.pt/dataset/nis-2023-008), BCNB (https://bupt-ai-cz.github.io/BCNB/), MUT-HET-RCC (https://aacrjournals.org/cancerres/article/82/15/2792/707325/ Intratumoral-Resolution-of-Driver-Gene-Mutation). Links for all public datasets are also presented in Supplementary Table 17 . Following institution policies, all requests for data collected or curated in-house will be evaluated on a case-by-case basis to determine whether the data requested is compliant with intellectual property and patient privacy obligations. Data can only be shared for academic research purposes and will require a material transfer agreement.', 'section': 'availability', 'sec_num': None}, {'text': 'Code and model weights for loading both TITAN and TITAN V can be accessed for academic research purposes at https://github.com/mahmoodlab/TITAN.', 'section': 'Code availability', 'sec_num': None}, {'text': \"Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\", 'section': 'annex', 'sec_num': None}, {'text': 'Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.', 'section': 'Reporting summary', 'sec_num': None}, {'text': 'T.D., S.J.W., A.H.S., R.J.C. and F.M. conceived the study and designed the experiments. L.P.L., T.D., R.J.C. and B.C. curated the Mass-340K WSIs and corresponding pathology reports. R.J.C., S.J.W., A.H.S., A.J.V., G.J., C.A.-P., P.D. and C.S.C. scanned the WSIs. T.D., S.J.W., R.J.C. and A.H.S. developed the stage 1 vision-only model. T.D., R.J.C., M.Y.L., S.J.W. and A.H.S. developed the stage 2 and stage 3 vision-language TITAN models. T.D., S.J.W. and M.Y.L. developed the codebase for zero-shot vision-language slide understanding. T.D., S.J.W., A.H.S., A.Z., A.J.V. and G.J. implemented the benchmarking codebase for pretrained slide models. A.K. and D.F.K.W. evaluated the synthetic captions and generated reports, and helped with the study design for slide retrieval. ', 'section': 'Author contributions', 'sec_num': None}, {'text': 'Extended data is available for this paper at https://doi.org/10.1038/s41591-025-03982-3.', 'section': 'Additional information', 'sec_num': None}, {'text': 'The online version contains supplementary material available at https://doi.org/10.1038/s41591-025-03982-3. and requests for materials be addressed to Long Phi Le or Faisal Mahmood.Peer review information Nature Medicine thanks Julien Calderaro, Olivier Elemento and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.Reprints and permissions information is available at www.nature.com/reprints.', 'section': 'Supplementary information', 'sec_num': None}, {'text': 'Linear models are fitted and evaluated on binary molecular status predictions for BCNB and MUT-HET. We observe that TITAN consistently performs best with + 0.9% on BCNB and MUT-HET, +1.7% on TCGA, and +3.7% on internal molecular classification of BRCA and LUAD, in averaged AUROC scores over the next best model PRISM. (b) Linear models are fitted and evaluated on five-fold splits on (c) The same models are evaluated on the corresponding external datasets from CPTAC and EBRAINS. (d) 6-level ER and PR prediction from Mass General Hospital (MGH) and 3-level PD-L1 prediction, all from immunohistochemistry slides. (e) Molecular classification tasks for BRCA and LUAD from Mass General Brigham (MGB). All error bars represent standard deviations based on bootstrapping (n = 1,000) or k-fold evaluation (k = 5).', 'section': 'Extended Data Fig. 2 | Linear probe results for molecular classification tasks.', 'sec_num': None}], 'ref_entries': {'FIGREF1': {'type_str': 'figure', 'text': 'Fig.2| TITAN evaluation. a, Impact of pretraining data size on TITAN V and baselines across four challenging subtyping tasks. TITAN V is pretrained with 12.5%, 25%, 50% and 100% of Mass-340K. b, The average performance of the four tasks against the number of parameters. c, Linear probe evaluation of TITAN and baselines on morphological classification, molecular status and survival prediction tasks. The mean pooling baseline uses the same patch encoder as TITAN (CONCHv1.5). Multiclass tasks are evaluated with balanced accuracy, binary tasks with AUROC and survival tasks with the concordance index. For external cohorts (DHMC, CPTAC), the classifier is trained on the corresponding TCGA cohort. All error bars represent s.d. based on bootstrapping (n = 1,000) or k-fold evaluation (k = 5). d, Ablation for positional encoding, number of transformer layers and inclusion of vision-pretraining stage. The performance', 'fig_num': '2', 'uris': None, 'num': None}, 'FIGREF3': {'type_str': 'figure', 'text': 'Fig. 3 | Visual-language evaluation of TITAN. a, A schematic for zero-shot evaluation. The query slide is classified by identifying the closest text prompt embedding in the slide embedding space. b, Zero-shot performance of TITAN and PRISM. All multiclass tasks are evaluated with balanced accuracy and binary tasks are evaluated with AUROC. All error bars represent s.d. based on bootstrapping (n = 1,000). Dashed lines represent average performance for respective models (red, TITAN; teal, PRISM) c, Ablation study comparing different pretraining strategies, and assessed with zero-shot performance averaged across TCGA-UT-8K, TCGA-OT, OT108 and EBRAINS. Evaluations are', 'fig_num': '3', 'uris': None, 'num': None}, 'FIGREF4': {'type_str': 'figure', 'text': 'the kidney shows papillary renal cell carcinoma, type 2, with Fuhrman grade 3 of 4... The slide from the breast shows an infiltrating ductal adenocarcinoma, moderately di erentiated... The slide from the right upper lobe of the lung shows an invasive mucinous adenocarcinoma, well di erentiated..', 'fig_num': None, 'uris': None, 'num': None}, 'FIGREF6': {'type_str': 'figure', 'text': 'Fig. 4 | Retrieval capabilities of TITAN. a, Slide retrieval results on rare cancer retrieval tasks assessed with Accuracy@K, with K = {1, 3, 5}. Rare-Cancer (internal rare cancer cohort) consists of TCGA, EBRAINS and the MGB internal cohort, with 43 rare and 143 common cancer types for a total of 186 classes. Rare-Cancer-Public (public rare cancer cohort) consists of TCGA and EBRAINS only, with 29 rare and 98 common cancer types for a total of 127 classes. Rare-Cancer-External consists of 12 rare cancer types for the ovary and soft tissue, curated at Kanagawa Cancer Center Hospital, Japan. b, Example of rare cancer retrieval on Rare-Cancer with the query slide and four representative retrieved slides. The number indicates the cosine similarity between the query and the retrieved', 'fig_num': '4', 'uris': None, 'num': None}, 'FIGREF7': {'type_str': 'figure', 'text': '-BRCA. TCGA-BRCA consists of 1,049 invasive breast carcinoma (BRCA) H&E FFPE diagnostic histopathology WSIs from TCGA. The WSIs are classified into the following two classes: invasive ductal carcinoma and invasive lobular carcinoma.TCGA-NSCLC. The TCGA-NSCLC consists of 1,043 H&E FFPE diagnosticWSIs from TCGA of 946 patients with NSCLC. The WSIs are classified into the following two classes: LUAD (531 slides) and lung squamous cell carcinoma (512 slides). We split the dataset into fivefold cross-validation, stratified by labels with a ratio of 60:20:20 (for exam-659:191:193 for fold 0). CPTAC-NSCLC serves as an external dataset 1,091 H&E FFPE diagnostic histopathology WSIs from CPTAC of 422 patients with NSCLC.TCGA-LUAD. The TCGA-LUAD consists of 524 H&E FFPE diagnosticWSIs from TCGA of 462 patients with LUAD. We predict the mutations in the genes EGFR (wild type (WT), 404 patients; https://doi.org/10.1038/s41591-025-03982-3 mutated (MUT), 58 patients), KRAS (WT, 317; MUT, 145), STK11 (WT, 391; MUT, 71) and TP53 222; MUT, We split the dataset fivefold cross-validation, stratified by labels with a ratio of 60:20:20 (for example, 659:191:193 for fold 0). CPTAC-LUAD serves as an external with 324 H&E FFPE diagnostic histopathology WSIs from CPTAC of 108 patients with LUAD.', 'fig_num': None, 'uris': None, 'num': None}, 'FIGREF8': {'type_str': 'figure', 'text': 'Attention heatmaps of TITAN. Exemplar attention heatmaps for three Transformer attention heads of TITAN (head #4, #10, #11) are shown across three different TCGA WSIs. Out of the 12 attention heads, we find that most attention heads focus on dense tumor regions, with certain attention heads such as head #10 focusing on tumor-adjacent stroma and head #11 focusing on non-tumor areas. Across different cancer types, while head #11 attends to tissue-specific morphologies such as peritumoral stroma in the thymoma WSI and the tumor-adjacent stroma and ducts in the BRCA WSI, we do observe that general morphological patterns such as tumor/non-tumor are conserved across tissue types. Extended Data Fig. 6 | Ablation experiments on different learning paradigms.', 'fig_num': '5', 'uris': None, 'num': None}, 'TABREF6': {'html': None, 'content': '<table><tr><td>Rare-Cancer-Public. Rare-Cancer. Rare-Cancer-External. BRACS consists of 547 H&amp;E FFPE WSIs of benign (including</td></tr><tr><td>normal), atypical and malignant breast tumors from 189 patients .</td></tr><tr><td>The cases are annotated in coarse and fine-grained subtypes of three</td></tr><tr><td>classes (benign tumors, 265; atypical tumors, 89; malignant tumors,</td></tr><tr><td>193) and six classes (atypical ductal hyperplasia, 48; ductal carcinoma</td></tr><tr><td>in situ, 61; flat epithelial atypia, 41; invasive carcinoma, 132; normal, 44;</td></tr><tr><td>pathological benign, 147; usual ductal hyperplasia, 74). We split the</td></tr><tr><td>dataset label-stratified at the patient level into five splits, with a ratio</td></tr><tr><td>of 60:20:20 (approximately 302:94:151 slides).</td></tr></table>', 'type_str': 'table', 'text': \"for a detailed overview the diagnosis distribution. The Rare-Cancer-Public is a pan-cancer dataset of H&E FFPE diagnostic WSIs from TCGA . The dataset consists of 1,982 WSIs, with 1,548 WSIs from TCGA and 434 WSIs from EBRAINS, representing 29 rare cancer types. According to the National Institute Health, rare cancers are defined as those occurring in fewer than 15 individuals per 100,000 annually44  The OncoTree codes of WSIs from TCGA and EBRAINS were manually curated for this criterion by two expert pathologists (A.K. and D.F.K.W.). EBRAINS provides more granular diagnostic classifications than the OncoTree codes, enabling the dataset to include finer distinctions for rare brain tumors. The dataset divided into five patient-level folds. To assess retrieval performance for rare cancers within a clinically representative dataset, we use one fold of the rare cancer dataset as the query set and the remaining folds with the common cancer types as a support set. In total, the support and query datasets contain 14,062 slides, including 11,646 WSIs from TCGA and 2,416 from EBRAINS. The Rare-Cancer is an in-house extension of the public dataset Rare-Cancer-Public with MGB internal cases. This dataset comprises 43 rare cancer types and 3,039 H&E FFPE diagnostic histopathology WSIs, where 1,056 additional cases were added from Brigham and Women's Hospital (BWH). The entire dataset, including common cancer types, comprises 19,626 WSIs, with 5,564 WSIs from BWH, covering 186 OncoTree codes. The Rare-Cancer-External is an external testcohort for rare cancer cases collected from the Department of Pathology, Kanagawa Cancer Center Hospital, Japan. This dataset of 39 H&E FFPE diagnostic WSIs from 12 rare ovarian and soft tissue cancers. The slides were stained using SAKURA TISSUE-TEK PRISMA 6130 Slide Stainer, and scanned by Leica Aperio AT2 at \u00d720 magnification. Detailed breakdown of the cohort can be found in Supplementary BCNB. The BCNB consists of 1,058 H&E FFPE WSIs of early breast cancer core-needle biopsies 141 . All cases are annotated with estrogen receptor (ER; WT, 227; MUT, 831), progesterone receptor (PR; WT, 268; MUT, 790) and HER2 (WT, 781; MUT, 277) expressions. We split the dataset label-stratified by a ratio of 60:20:20 (676:170:212 slides).\", 'num': None}, 'TABREF7': {'html': None, 'content': '<table/>', 'type_str': 'table', 'text': '780:1,620 slides). The test set is balacross the classes contains 15 slides class. The PANDA consists of 10,616 H&E FFPE diagnostic histopathology WSIs of core-needle biopsies of prostate cancer sourced from Radboud University Medical Center and the Karolinska Institute. Each slide is assigned a score recommended by the International Sociof Urological Pathology (ISUP) that defines prostate cancer grade (six-class grading task). For quality control, we follow prior work in excluding slides that were erroneously annotated or had noisy labels, resulting in an overall 9,555 slides (grade 0, 2,603; grade 1, 2,399; grade 2, 1,209; grade 3, 1,118; grade 4, 1,124; grade 5, 1,102). For training and we label-stratified PANDA into 80:10:10 train-val-test folds (7,645:954:953 slides).', 'num': None}}}}\n\n## Task\n1. We want to reproduce the method described in the attached paper. \n2. The authors did not release any official code, so we have to plan our own implementation.\n3. Before writing any Python code, please outline a comprehensive plan that covers:\n   - Key details from the paper's **Methodology**.\n   - Important aspects of **Experiments**, including dataset requirements, experimental settings, hyperparameters, or evaluation metrics.\n4. The plan should be as **detailed and informative** as possible to help us write the final code later.\n\n## Requirements\n- You don't need to provide the actual code yet; focus on a **thorough, clear strategy**.\n- If something is unclear from the paper, mention it explicitly.\n\n## Instruction\nThe response should give us a strong roadmap, making it easier to write the code later."}, {"role": "assistant", "content": "**Reproduction Roadmap (TITAN, no official code)**\n\n## 1. Define the reproduction scope first\n1. Reproduce the full pipeline in three stages:\n- Stage 1: vision-only slide pretraining (`TITAN V`) with iBOT in feature space.\n- Stage 2: ROI-caption vision-language alignment (CoCa).\n- Stage 3: WSI-report vision-language alignment (CoCa), yielding `TITAN`.\n\n2. Split deliverables into two tracks:\n- Core reproducibility: architecture, pretraining recipe, and key evaluations.\n- Practical reproducibility: public-data-only variant (since Mass-340K and many internal cohorts are inaccessible).\n\n---\n\n## 2. Methodology implementation plan\n\n### 2.1 Data pipeline and preprocessing\n1. Implement WSI preprocessing (CLAM-style):\n- Tissue segmentation on low-res HSV saturation channel.\n- Median blur, morphological closing, contour-area filtering.\n- Tile non-overlapping `512x512` patches at `20x`.\n- Extract per-patch `768-d` features using `CONCHv1.5`.\n\n2. Build spatial feature grids:\n- Place patch features into 2D grid by original WSI coordinates.\n- Keep background mask for non-tissue locations.\n- Support variable-size grids per WSI.\n\n3. Tissue-group preprocessing (for stage 1 sampling):\n- Cluster segmented contours by spatial proximity.\n- Filter groups with `<16` patches.\n- Save per-group coordinate/index metadata.\n\n### 2.2 TITAN V architecture (stage 1 backbone)\n1. Vision encoder:\n- ViT-style encoder with `6` transformer blocks.\n- `12` attention heads, head dim `64`.\n- Embedding dim `768`, MLP hidden dim `3072`.\n- Replace pixel patch-embedding layer with MLP for feature tokens.\n\n2. Positional encoding:\n- Implement 2D ALiBi bias in attention using Euclidean distance between token coordinates (coordinates normalized by patch size `512` as described).\n- Keep compatibility with train-short/test-long extrapolation.\n\n### 2.3 Stage 1 training: iBOT in feature space\n1. Sampling and views:\n- Randomly sample `16x16` region crops from tissue groups (`8192x8192` context at 20x).\n- From each region crop create `2` global views (`14x14`) and `10` local views (`6x6`), no resize interpolation.\n\n2. Augmentations:\n- Horizontal/vertical flips.\n- Feature-space posterization augmentation (paper cites frozen feature augmentation).\n\n3. Training setup (reported):\n- `270` epochs (`91,260` iterations).\n- `4 x A100 80GB`, DDP.\n- Local batch `256/GPU` (effective `1024`).\n- Save checkpoints for later stage initialization and ablations.\n\n### 2.4 Stage 2/3 multimodal CoCa continual pretraining\n1. Multimodal model components:\n- Image encoder initialized from `TITAN V`.\n- Add two attentional poolers:\n- Single-query contrastive pooler for global embedding.\n- `128`-query reconstruction pooler for decoder interaction.\n- Text encoder + multimodal decoder initialized from CONCHv1.5 (12 layers, dim 768, hidden 3072).\n\n2. Stage 2 (ROI-caption):\n- Train on `423,122` pairs of `8192x8192` ROI and synthetic captions.\n- Reported compute: `8 x A100 80GB`, local batch `196/GPU`, grad accum `2`, effective batch `3136`.\n\n3. Stage 3 (WSI-report):\n- Train on `182,862` WSI-report pairs.\n- Random crop WSI grids to `64x64` tokens (`32768x32768` px equivalent).\n- Reported setup: local batch `16/GPU`, grad accum `2`, effective batch `256`.\n- Use reduced LR/WD + slow warmup for vision backbone to avoid degradation.\n\n---\n\n## 3. Dataset and experiment plan\n\n## 3.1 Pretraining data requirements\n1. Target (paper): Mass-340K (`335,645` WSIs, 20 organs, mixed stains/scanners) + synthetic captions + paired reports.\n2. Reality: most data is internal; create two reproducibility tiers:\n- Tier A (closest): institutional/private data agreements + GTEx/TCGA additions.\n- Tier B (public-only): GTEx/TCGA/CPTAC/EBRAINS and other public pathology sets with same preprocessing and staged training logic.\n\n## 3.2 Downstream task suite to implement\n1. Linear probe tasks:\n- Multiclass morphology/subtyping.\n- Binary molecular markers.\n- Ordinal grading.\n- Survival prediction.\n\n2. Few-shot:\n- K in `{1,2,4,8,16,32}`.\n- 50 random runs, fixed test split.\n\n3. Retrieval:\n- Unimodal slide retrieval: Acc@1/3/5, MVAcc@5.\n- Cross-modal retrieval: Recall@1/3/5/10 + mean recall.\n\n4. Zero-shot classification:\n- Prompt-ensemble text embeddings per class.\n- Classify by nearest text embedding (cosine / equivalent normalized dot product).\n\n5. Report generation:\n- Zero-shot generation with beam search (`num_beams=5`, `num_beam_groups=1`).\n- Metrics: METEOR, ROUGE-1, BLEU-1.\n\n## 3.3 Baselines to reproduce\n1. Mean pooling on same patch features.\n2. ABMIL supervised baseline.\n3. If available: PRISM, GigaPath, CHIEF (public checkpoints/code constraints apply).\n4. Fine-tuning comparisons:\n- Pretrained TITAN/TITAN V fine-tune vs random init fine-tune.\n\n---\n\n## 4. Exact evaluation settings to follow\n1. Linear probing:\n- scikit-learn logistic regression (L-BFGS).\n- L2 search over 45 log-spaced values from `1e-6` to `10`.\n- Max iter `500`.\n- No-validation/few-shot setting: L2=`1`, iter=`1000`.\n\n2. k-NN probing:\n- SimpleShot prototypes (class mean embeddings).\n- k-NN with `k=20`.\n- Center + L2-normalize embeddings; Euclidean distance.\n\n3. Survival:\n- Linear Cox model (`scikit-survival`), disease-specific survival endpoint.\n- Five-fold site-preserved stratification where applicable.\n- Hyperparameter search for regularization over log-spaced grid (exact range partially unclear from parse; see ambiguity list).\n\n4. Statistics/reporting:\n- 5-fold datasets: mean \u00b1 SD across folds.\n- Single split: bootstrap (`n=1000`).\n- GLMM (beta family) for multi-dataset method comparisons + Tukey-corrected Wald tests.\n\n---\n\n## 5. Execution order for implementation (before coding details)\n1. Lock reproducibility scope (full vs public-only).\n2. Build data preprocessing + feature extraction pipeline.\n3. Implement TITAN V + stage 1 iBOT training.\n4. Implement CoCa multimodal stack and stage 2/3 training.\n5. Implement shared embedding inference API (slide/text/report modes).\n6. Implement evaluation harnesses (linear, few-shot, retrieval, zero-shot, report generation, survival).\n7. Run ablations:\n- ALiBi vs absolute PE.\n- 4/6/12 layers.\n- Region size (`4096^2` vs `8192^2`).\n- With/without stage 1 pretraining.\n8. Reproduce headline tables/figures with confidence intervals and significance tests.\n\n---\n\n## 6. What is unclear or missing from the paper\n1. Many critical hyperparameters are only in Supplementary Tables (2,3,4,5,6,7,8,9,10, etc.), not in provided text.\n2. Exact iBOT settings are incomplete: optimizer, LR schedule, EMA/teacher temperatures, mask ratio/details, weight decay, etc.\n3. Exact 2D ALiBi slope schedule per head is not fully specified in the excerpt.\n4. \u201cPosterization feature augmentation\u201d implementation details are under-specified.\n5. Tissue-group clustering algorithm details and thresholds are not fully specified.\n6. Synthetic caption/report-cleaning prompts (PathChat + Qwen2) are referenced but not fully included.\n7. Rare-Cancer and many benchmark datasets are internal/private or partially inaccessible.\n8. Some numeric ranges in parsed methods appear OCR-corrupted (for example Cox regularization search range).\n\nIf you want, the next step is a concrete implementation spec document (module-by-module APIs, config schema, and training/eval command matrix) so coding can start directly."}, {"role": "user", "content": "Your goal is to create a concise, usable, and complete software system design for reproducing the paper's method. Use appropriate open-source libraries and keep the overall architecture simple.\n             \nBased on the plan for reproducing the paper\u2019s main method, please design a concise, usable, and complete software system. \nKeep the architecture simple and make effective use of open-source libraries.\n\n-----\n\n## Format Example\n[CONTENT]\n{\n    \"Implementation approach\": \"We will ... ,\n    \"File list\": [\n        \"main.py\",  \n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\",\n        \"evaluation.py\" \n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Main {\n        +__init__()\n        +run_experiment()\n    }\n    class DatasetLoader {\n        +__init__(config: dict)\n        +load_data() -> Any\n    }\n    class Model {\n        +__init__(params: dict)\n        +forward(x: Tensor) -> Tensor\n    }\n    class Trainer {\n        +__init__(model: Model, data: Any)\n        +train() -> None\n    }\n    class Evaluation {\n        +__init__(model: Model, data: Any)\n        +evaluate() -> dict\n    }\n    Main --> DatasetLoader\n    Main --> Trainer\n    Main --> Evaluation\n    Trainer --> Model\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant M as Main\n    participant DL as DatasetLoader\n    participant MD as Model\n    participant TR as Trainer\n    participant EV as Evaluation\n    M->>DL: load_data()\n    DL-->>M: return dataset\n    M->>MD: initialize model()\n    M->>TR: train(model, dataset)\n    TR->>MD: forward(x)\n    MD-->>TR: predictions\n    TR-->>M: training complete\n    M->>EV: evaluate(model, dataset)\n    EV->>MD: forward(x)\n    MD-->>EV: predictions\n    EV-->>M: metrics\n\",\n    \"Anything UNCLEAR\": \"Need clarification on the exact dataset format and any specialized hyperparameters.\"\n}\n[/CONTENT]\n\n## Nodes: \"<node>: <type>  # <instruction>\"\n- Implementation approach: <class 'str'>  # Summarize the chosen solution strategy.\n- File list: typing.List[str]  # Only need relative paths. ALWAYS write a main.py or app.py here.\n- Data structures and interfaces: typing.Optional[str]  # Use mermaid classDiagram code syntax, including classes, method(__init__ etc.) and functions with type annotations, CLEARLY MARK the RELATIONSHIPS between classes, and comply with PEP8 standards. The data structures SHOULD BE VERY DETAILED and the API should be comprehensive with a complete design.\n- Program call flow: typing.Optional[str] # Use sequenceDiagram code syntax, COMPLETE and VERY DETAILED, using CLASSES AND API DEFINED ABOVE accurately, covering the CRUD AND INIT of each object, SYNTAX MUST BE CORRECT.\n- Anything UNCLEAR: <class 'str'>  # Mention ambiguities and ask for clarifications.\n\n## Constraint\nFormat: output wrapped inside [CONTENT][/CONTENT] like the format example, nothing else.\n\n## Action\nFollow the instructions for the nodes, generate the output, and ensure it follows the format example."}, {"role": "assistant", "content": "[CONTENT]\n{\n  \"Implementation approach\": \"We will build a single Python package with a config-driven pipeline that supports: (1) WSI preprocessing and patch-feature extraction, (2) stage-1 TITAN-V pretraining with iBOT-style SSL in feature space, (3) stage-2/3 CoCa-style vision-language alignment, and (4) unified downstream evaluation (linear/few-shot/zero-shot/retrieval/report-generation/survival). Architecture is intentionally simple: one entrypoint, one registry-based dataset/model/trainer layer, and one reusable embedding API. Open-source stack: PyTorch, timm, open_clip, OpenSlide, CLAM-style preprocessing utilities, scikit-learn, scikit-survival, faiss, pandas, numpy, hydra, pydantic, matplotlib/seaborn.\",\n  \"File list\": [\n    \"main.py\",\n    \"configs/default.yaml\",\n    \"configs/data/public_repro.yaml\",\n    \"configs/model/titan_v.yaml\",\n    \"configs/model/titan_multimodal.yaml\",\n    \"configs/train/stage1_ibot.yaml\",\n    \"configs/train/stage2_coca.yaml\",\n    \"configs/train/stage3_coca.yaml\",\n    \"configs/eval/linear_probe.yaml\",\n    \"configs/eval/few_shot.yaml\",\n    \"configs/eval/zero_shot.yaml\",\n    \"configs/eval/retrieval.yaml\",\n    \"configs/eval/report_generation.yaml\",\n    \"configs/eval/survival.yaml\",\n    \"src/core/config_schema.py\",\n    \"src/core/registry.py\",\n    \"src/core/utils.py\",\n    \"src/core/logging_utils.py\",\n    \"src/data/wsi_reader.py\",\n    \"src/data/segment_tissue.py\",\n    \"src/data/tile_wsi.py\",\n    \"src/data/extract_patch_features.py\",\n    \"src/data/build_feature_grid.py\",\n    \"src/data/tissue_grouping.py\",\n    \"src/data/caption_report_processing.py\",\n    \"src/data/datasets.py\",\n    \"src/data/collate.py\",\n    \"src/models/alibi_2d.py\",\n    \"src/models/titan_encoder.py\",\n    \"src/models/ibot_heads.py\",\n    \"src/models/coca_multimodal.py\",\n    \"src/models/text_modules.py\",\n    \"src/models/losses.py\",\n    \"src/train/base_trainer.py\",\n    \"src/train/stage1_trainer.py\",\n    \"src/train/stage2_trainer.py\",\n    \"src/train/stage3_trainer.py\",\n    \"src/eval/embed_api.py\",\n    \"src/eval/linear_probe.py\",\n    \"src/eval/knn_probe.py\",\n    \"src/eval/few_shot.py\",\n    \"src/eval/zero_shot.py\",\n    \"src/eval/retrieval.py\",\n    \"src/eval/report_generation.py\",\n    \"src/eval/survival.py\",\n    \"src/eval/statistics.py\",\n    \"src/baselines/mean_pooling.py\",\n    \"src/baselines/abmil.py\",\n    \"scripts/run_stage1.py\",\n    \"scripts/run_stage2.py\",\n    \"scripts/run_stage3.py\",\n    \"scripts/run_eval.py\",\n    \"scripts/prepare_public_data.py\",\n    \"tests/test_alibi_2d.py\",\n    \"tests/test_feature_grid.py\",\n    \"tests/test_losses.py\",\n    \"tests/test_eval_metrics.py\"\n  ],\n  \"Data structures and interfaces\": \"```mermaid\\nclassDiagram\\n    class MainApp {\\n        +__init__(cfg_path: str) None\\n        +build_components() None\\n        +run() None\\n        +run_stage(stage: str) None\\n        +run_eval(eval_name: str) None\\n    }\\n\\n    class ExperimentConfig {\\n        +seed: int\\n        +output_dir: str\\n        +device: str\\n        +num_workers: int\\n        +from_yaml(path: str) ExperimentConfig\\n        +validate() None\\n    }\\n\\n    class DataConfig {\\n        +wsi_root: str\\n        +meta_csv: str\\n        +patch_size: int\\n        +magnification: int\\n        +feature_dim: int\\n        +splits_path: str\\n    }\\n\\n    class ModelConfig {\\n        +embed_dim: int\\n        +num_layers: int\\n        +num_heads: int\\n        +mlp_dim: int\\n        +use_alibi_2d: bool\\n        +max_tokens_train: int\\n    }\\n\\n    class TrainConfig {\\n        +stage: str\\n        +epochs: int\\n        +lr: float\\n        +weight_decay: float\\n        +batch_size: int\\n        +grad_accum_steps: int\\n        +mixed_precision: bool\\n    }\\n\\n    class Registry {\\n        +register_dataset(name: str, cls: type) None\\n        +register_model(name: str, cls: type) None\\n        +register_trainer(name: str, cls: type) None\\n        +create_dataset(name: str, cfg: DataConfig) BaseDataset\\n        +create_model(name: str, cfg: ModelConfig) torch.nn.Module\\n        +create_trainer(name: str, cfg: TrainConfig) BaseTrainer\\n    }\\n\\n    class WSIReader {\\n        +__init__(backend: str = \\\"openslide\\\") None\\n        +open(path: str) Any\\n        +read_region(path: str, x: int, y: int, size: int, level: int) np.ndarray\\n        +get_mpp(path: str) float\\n        +get_dimensions(path: str) tuple~int, int~\\n    }\\n\\n    class TissueSegmenter {\\n        +__init__(sat_thresh: float, min_area: int) None\\n        +segment_thumbnail(wsi_thumb: np.ndarray) np.ndarray\\n        +postprocess_mask(mask: np.ndarray) np.ndarray\\n        +extract_contours(mask: np.ndarray) list\\n    }\\n\\n    class WSITiler {\\n        +__init__(patch_size: int, stride: int) None\\n        +tile_from_mask(wsi_path: str, mask: np.ndarray) list~tuple~int,int~~\\n        +filter_background(coords: list, min_tissue_ratio: float) list\\n    }\\n\\n    class PatchFeatureExtractor {\\n        +__init__(encoder_name: str, ckpt_path: str, out_dim: int = 768) None\\n        +extract_batch(images: torch.Tensor) torch.Tensor\\n        +extract_wsi(coords: list, wsi_path: str) np.ndarray\\n        +save_features(out_path: str, feats: np.ndarray, coords: np.ndarray) None\\n    }\\n\\n    class FeatureGridBuilder {\\n        +__init__(patch_size: int = 512) None\\n        +build_grid(feats: np.ndarray, coords: np.ndarray) FeatureGrid\\n        +build_background_mask(coords: np.ndarray, shape: tuple~int,int~) np.ndarray\\n        +crop_grid(grid: FeatureGrid, crop_hw: tuple~int,int~, random: bool = True) FeatureGrid\\n    }\\n\\n    class TissueGrouper {\\n        +__init__(min_patches: int = 16, method: str = \\\"dbscan\\\") None\\n        +group_contours(contours: list) list\\n        +filter_small_groups(groups: list) list\\n        +save_groups(path: str, groups: list) None\\n    }\\n\\n    class FeatureGrid {\\n        +features: torch.Tensor\\n        +coords_xy: torch.Tensor\\n        +valid_mask: torch.Tensor\\n        +slide_id: str\\n        +to(device: str) FeatureGrid\\n    }\\n\\n    class TITANEncoder {\\n        +__init__(cfg: ModelConfig) None\\n        +forward(grid_tokens: torch.Tensor, pos_xy: torch.Tensor, attn_mask: torch.Tensor) torch.Tensor\\n        +encode_slide(grid: FeatureGrid) torch.Tensor\\n        +load_pretrained(path: str) None\\n        +save(path: str) None\\n    }\\n\\n    class ALiBi2D {\\n        +__init__(num_heads: int, slopes: torch.Tensor) None\\n        +build_bias(coords_xy: torch.Tensor) torch.Tensor\\n        +apply(attn_scores: torch.Tensor, bias: torch.Tensor) torch.Tensor\\n    }\\n\\n    class IBOTHead {\\n        +__init__(embed_dim: int, out_dim: int) None\\n        +forward(tokens: torch.Tensor) torch.Tensor\\n    }\\n\\n    class IBOTLoss {\\n        +__init__(student_temp: float, teacher_temp: float, center_momentum: float) None\\n        +compute(student_out: torch.Tensor, teacher_out: torch.Tensor, mask: torch.Tensor) torch.Tensor\\n        +update_center(teacher_out: torch.Tensor) None\\n    }\\n\\n    class CoCaModel {\\n        +__init__(vision: TITANEncoder, text_encoder: TextEncoder, decoder: MultimodalDecoder) None\\n        +forward_contrastive(batch: MultimodalBatch) dict\\n        +forward_captioning(batch: MultimodalBatch) dict\\n        +encode_image(grid: FeatureGrid) torch.Tensor\\n        +encode_text(input_ids: torch.Tensor, attn_mask: torch.Tensor) torch.Tensor\\n        +generate_report(grid: FeatureGrid, num_beams: int = 5) str\\n    }\\n\\n    class TextEncoder {\\n        +__init__(name: str, vocab_size: int, max_len: int) None\\n        +forward(input_ids: torch.Tensor, attn_mask: torch.Tensor) torch.Tensor\\n    }\\n\\n    class MultimodalDecoder {\\n        +__init__(num_layers: int, embed_dim: int) None\\n        +forward(image_tokens: torch.Tensor, input_ids: torch.Tensor) torch.Tensor\\n        +generate(image_tokens: torch.Tensor, num_beams: int, max_new_tokens: int) torch.Tensor\\n    }\\n\\n    class BaseDataset {\\n        +__len__() int\\n        +__getitem__(idx: int) dict\\n    }\\n\\n    class Stage1Dataset {\\n        +__init__(meta_csv: str, groups_path: str, crop_size: int = 16) None\\n        +sample_region_crop(slide_id: str) FeatureGrid\\n        +make_ibot_views(region_grid: FeatureGrid) dict\\n    }\\n\\n    class Stage2Dataset {\\n        +__init__(pairs_csv: str, tokenizer_name: str) None\\n        +__getitem__(idx: int) MultimodalBatch\\n    }\\n\\n    class Stage3Dataset {\\n        +__init__(pairs_csv: str, tokenizer_name: str, crop_hw: tuple~int,int~ = (64, 64)) None\\n        +__getitem__(idx: int) MultimodalBatch\\n    }\\n\\n    class MultimodalBatch {\\n        +image_grid: FeatureGrid\\n        +input_ids: torch.Tensor\\n        +attention_mask: torch.Tensor\\n        +labels: torch.Tensor\\n        +slide_id: str\\n    }\\n\\n    class BaseTrainer {\\n        +__init__(model: torch.nn.Module, cfg: TrainConfig) None\\n        +fit(train_loader: DataLoader, val_loader: DataLoader | None = None) None\\n        +train_step(batch: dict) dict\\n        +validate_step(batch: dict) dict\\n        +save_checkpoint(path: str) None\\n        +load_checkpoint(path: str) None\\n    }\\n\\n    class Stage1Trainer {\\n        +__init__(model: TITANEncoder, ibot_head: IBOTHead, loss_fn: IBOTLoss, cfg: TrainConfig) None\\n        +train_step(batch: dict) dict\\n    }\\n\\n    class Stage2Trainer {\\n        +__init__(model: CoCaModel, cfg: TrainConfig) None\\n        +train_step(batch: MultimodalBatch) dict\\n    }\\n\\n    class Stage3Trainer {\\n        +__init__(model: CoCaModel, cfg: TrainConfig) None\\n        +train_step(batch: MultimodalBatch) dict\\n    }\\n\\n    class EmbeddingService {\\n        +__init__(vision_model: TITANEncoder, text_model: TextEncoder | None = None) None\\n        +embed_slides(slide_ids: list~str~) np.ndarray\\n        +embed_reports(texts: list~str~) np.ndarray\\n        +center_and_l2(x: np.ndarray, mean_vec: np.ndarray | None = None) tuple~np.ndarray, np.ndarray~\\n    }\\n\\n    class Evaluator {\\n        +run_linear_probe(features: np.ndarray, y: np.ndarray, split: dict) dict\\n        +run_knn_probe(features: np.ndarray, y: np.ndarray, k: int = 20) dict\\n        +run_few_shot(features: np.ndarray, y: np.ndarray, shots: list~int~, runs: int = 50) dict\\n        +run_zero_shot(slide_emb: np.ndarray, class_text_emb: np.ndarray, y: np.ndarray) dict\\n        +run_slide_retrieval(query: np.ndarray, db: np.ndarray, yq: np.ndarray, ydb: np.ndarray) dict\\n        +run_cross_modal_retrieval(slide_emb: np.ndarray, report_emb: np.ndarray, labels: np.ndarray) dict\\n        +run_report_generation(model: CoCaModel, dataset: BaseDataset) dict\\n        +run_survival(features: np.ndarray, time: np.ndarray, event: np.ndarray, folds: list) dict\\n    }\\n\\n    class StatsAnalyzer {\\n        +bootstrap_metrics(y_true: np.ndarray, y_pred: np.ndarray, n_boot: int = 1000) dict\\n        +fold_mean_std(scores: list~float~) dict\\n        +fit_glmm(results_df: pd.DataFrame) Any\\n        +pairwise_tests_glmm(model: Any, method: str = \\\"tukey\\\") pd.DataFrame\\n    }\\n\\n    MainApp --> ExperimentConfig\\n    MainApp --> Registry\\n    MainApp --> BaseTrainer\\n    MainApp --> Evaluator\\n    Registry --> Stage1Dataset\\n    Registry --> Stage2Dataset\\n    Registry --> Stage3Dataset\\n    Registry --> TITANEncoder\\n    Registry --> CoCaModel\\n    BaseTrainer --> TITANEncoder\\n    Stage1Trainer --> IBOTHead\\n    Stage1Trainer --> IBOTLoss\\n    Stage2Trainer --> CoCaModel\\n    Stage3Trainer --> CoCaModel\\n    WSIReader --> TissueSegmenter\\n    TissueSegmenter --> WSITiler\\n    WSITiler --> PatchFeatureExtractor\\n    PatchFeatureExtractor --> FeatureGridBuilder\\n    FeatureGridBuilder --> FeatureGrid\\n    TITANEncoder --> ALiBi2D\\n    CoCaModel --> TITANEncoder\\n    CoCaModel --> TextEncoder\\n    CoCaModel --> MultimodalDecoder\\n    Evaluator --> EmbeddingService\\n    Evaluator --> StatsAnalyzer\\n```\",\n  \"Program call flow\": \"```mermaid\\nsequenceDiagram\\n    autonumber\\n    participant U as User\\n    participant M as MainApp\\n    participant C as ExperimentConfig\\n    participant R as Registry\\n    participant DR as DataPipeline(WSIReader/Segment/Tiler/Extractor/Grid)\\n    participant D1 as Stage1Dataset\\n    participant D2 as Stage2Dataset\\n    participant D3 as Stage3Dataset\\n    participant V as TITANEncoder\\n    participant I as Stage1Trainer\\n    participant CM as CoCaModel\\n    participant T2 as Stage2Trainer\\n    participant T3 as Stage3Trainer\\n    participant E as EmbeddingService\\n    participant EV as Evaluator\\n    participant S as StatsAnalyzer\\n    participant FS as FileStore(checkpoints/artifacts)\\n\\n    U->>M: python main.py --config configs/default.yaml --mode train_stage1\\n    M->>C: from_yaml(path)\\n    C-->>M: validated cfg\\n    M->>R: register/create components\\n\\n    rect rgb(245,245,245)\\n    Note over M,DR: Data preparation CRUD\\n    M->>DR: read WSI metadata (Create dataset index)\\n    DR->>DR: segment tissue masks\\n    DR->>DR: tile 512x512 @20x\\n    DR->>DR: extract 768-d patch features\\n    DR->>DR: build/save feature grids + masks\\n    DR->>FS: write .h5/.pt features (Create)\\n    M->>DR: build tissue groups (Create)\\n    DR->>FS: persist groups.json (Create)\\n    M->>DR: verify/read cached artifacts (Read)\\n    DR-->>M: ready paths\\n    end\\n\\n    rect rgb(235,245,255)\\n    Note over M,I: Stage 1 TITAN-V training\\n    M->>R: create_dataset(stage1)\\n    R-->>M: D1\\n    M->>R: create_model(titan_encoder)\\n    R-->>M: V\\n    M->>R: create_trainer(stage1)\\n    R-->>M: I\\n    M->>I: fit(D1)\\n    loop each epoch/iter\\n        I->>D1: __getitem__() -> region crop\\n        D1-->>I: 2 global + 10 local views\\n        I->>V: forward(student/teacher views)\\n        V-->>I: token embeddings\\n        I->>I: compute iBOT loss + EMA teacher update\\n        I->>I: optimizer/scheduler step\\n    end\\n    I->>FS: save titan_v.ckpt (Create/Update)\\n    end\\n\\n    U->>M: python main.py --mode train_stage2 --init titan_v.ckpt\\n    rect rgb(235,255,235)\\n    Note over M,T2: Stage 2 ROI-caption CoCa alignment\\n    M->>R: create_dataset(stage2)\\n    R-->>M: D2\\n    M->>R: create_model(coca_multimodal, init=titan_v)\\n    R-->>M: CM\\n    M->>R: create_trainer(stage2)\\n    R-->>M: T2\\n    M->>T2: fit(D2)\\n    loop each epoch/iter\\n        T2->>D2: __getitem__() -> MultimodalBatch\\n        D2-->>T2: image grid + tokenized caption\\n        T2->>CM: forward_contrastive(batch)\\n        CM-->>T2: contrastive logits + decoder logits\\n        T2->>T2: contrastive + caption loss backward\\n    end\\n    T2->>FS: save titan_stage2.ckpt (Create/Update)\\n    end\\n\\n    U->>M: python main.py --mode train_stage3 --init titan_stage2.ckpt\\n    rect rgb(255,245,235)\\n    Note over M,T3: Stage 3 WSI-report CoCa alignment\\n    M->>R: create_dataset(stage3)\\n    R-->>M: D3\\n    M->>R: create_trainer(stage3)\\n    R-->>M: T3\\n    M->>T3: fit(D3)\\n    loop each epoch/iter\\n        T3->>D3: __getitem__() -> 64x64 crop + report tokens\\n        T3->>CM: forward_contrastive + forward_captioning\\n        CM-->>T3: losses\\n        T3->>T3: low-LR vision backbone update\\n    end\\n    T3->>FS: save titan_final.ckpt (Create/Update)\\n    end\\n\\n    U->>M: python main.py --mode eval --ckpt titan_final.ckpt\\n    M->>E: init(vision_model, text_model)\\n    M->>EV: run all selected evaluations\\n\\n    par Linear/kNN/Few-shot\\n        EV->>E: embed_slides(train/val/test)\\n        E-->>EV: normalized embeddings\\n        EV->>EV: linear_probe, knn_probe, few_shot\\n    and Zero-shot\\n        EV->>E: embed class prompts with text encoder\\n        EV->>EV: nearest class by cosine similarity\\n    and Slide retrieval\\n        EV->>E: embed query/database slides\\n        EV->>EV: Acc@1/3/5, MVAcc@5\\n    and Cross-modal retrieval\\n        EV->>E: embed slides + reports\\n        EV->>EV: Recall@1/3/5/10, mean recall\\n    and Report generation\\n        EV->>CM: generate_report(num_beams=5)\\n        CM-->>EV: generated text\\n        EV->>EV: METEOR/ROUGE-1/BLEU-1\\n    and Survival\\n        EV->>E: patient-level embedding aggregation\\n        EV->>EV: Cox PH with fold protocol\\n    end\\n\\n    EV->>S: bootstrap/fold aggregation + GLMM pairwise tests\\n    S-->>EV: statistical summary\\n    EV->>FS: write metrics.json + plots.csv/png (Create/Update)\\n    EV-->>M: consolidated report\\n    M-->>U: done\\n```\",\n  \"Anything UNCLEAR\": \"Unclear from the paper text alone: exact Supplementary hyperparameters (optimizer betas, LR schedule details, iBOT temperatures/centering/mask ratios, ALiBi per-head slopes, stage-2/3 loss weights, warmup lengths), exact tissue-group clustering thresholds, exact prompt templates for synthetic caption/report cleaning/rewriting, and full access to internal datasets (Mass-340K, Rare-Cancer internal, several MGB cohorts). Please confirm whether we should target a public-data-only reproduction first and treat missing hyperparameters as tunable defaults in config.\"\n}\n[/CONTENT]"}, {"role": "user", "content": "Your goal is break down tasks according to PRD/technical design, generate a task list, and analyze task dependencies. \nYou will break down tasks, analyze dependencies.\n             \nYou outline a clear PRD/technical design for reproducing the paper\u2019s method and experiments. \n\nNow, let's break down tasks according to PRD/technical design, generate a task list, and analyze task dependencies.\nThe Logic Analysis should not only consider the dependencies between files but also provide detailed descriptions to assist in writing the code needed to reproduce the paper.\n\n-----\n\n## Format Example\n[CONTENT]\n{\n    \"Required packages\": [\n        \"numpy==1.21.0\",\n        \"torch==1.9.0\"  \n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"data_preprocessing.py\",\n            \"DataPreprocessing class ........\"\n        ],\n        [\n            \"trainer.py\",\n            \"Trainer ....... \"\n        ],\n        [\n            \"dataset_loader.py\",\n            \"Handles loading and ........\"\n        ],\n        [\n            \"model.py\",\n            \"Defines the model .......\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluation class ........ \"\n        ],\n        [\n            \"main.py\",\n            \"Entry point  .......\"\n        ]\n    ],\n    \"Task list\": [\n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\", \n        \"evaluation.py\",\n        \"main.py\"  \n    ],\n    \"Full API spec\": \"openapi: 3.0.0 ...\",\n    \"Shared Knowledge\": \"Both data_preprocessing.py and trainer.py share ........\",\n    \"Anything UNCLEAR\": \"Clarification needed on recommended hardware configuration for large-scale experiments.\"\n}\n\n[/CONTENT]\n\n## Nodes: \"<node>: <type>  # <instruction>\"\n- Required packages: typing.Optional[typing.List[str]]  # Provide required third-party packages in requirements.txt format.(e.g., 'numpy==1.21.0').\n- Required Other language third-party packages: typing.List[str]  # List down packages required for non-Python languages. If none, specify \"No third-party dependencies required\".\n- Logic Analysis: typing.List[typing.List[str]]  # Provide a list of files with the classes/methods/functions to be implemented, including dependency analysis and imports. Include as much detailed description as possible.\n- Task list: typing.List[str]  # Break down the tasks into a list of filenames, prioritized based on dependency order. The task list must include the previously generated file list.\n- Full API spec: <class 'str'>  # Describe all APIs using OpenAPI 3.0 spec that may be used by both frontend and backend. If front-end and back-end communication is not required, leave it blank.\n- Shared Knowledge: <class 'str'>  # Detail any shared knowledge, like common utility functions or configuration variables.\n- Anything UNCLEAR: <class 'str'>  # Mention any unresolved questions or clarifications needed from the paper or project scope.\n\n## Constraint\nFormat: output wrapped inside [CONTENT][/CONTENT] like the format example, nothing else.\n\n## Action\nFollow the node instructions above, generate your output accordingly, and ensure it follows the given format example."}, {"role": "assistant", "content": "[CONTENT]\n{\n  \"Required packages\": [\n    \"torch==2.0.1\",\n    \"torchvision==0.15.2\",\n    \"timm==0.9.16\",\n    \"open-clip-torch==2.24.0\",\n    \"numpy==1.24.4\",\n    \"pandas==2.0.3\",\n    \"scikit-learn==1.2.2\",\n    \"scikit-survival==0.23.1\",\n    \"scipy==1.10.1\",\n    \"faiss-cpu==1.7.4\",\n    \"openslide-python==1.2.0\",\n    \"h5py==3.10.0\",\n    \"opencv-python==4.9.0.80\",\n    \"Pillow==10.2.0\",\n    \"hydra-core==1.3.2\",\n    \"omegaconf==2.3.0\",\n    \"pydantic==2.6.1\",\n    \"pyyaml==6.0.1\",\n    \"tqdm==4.66.2\",\n    \"matplotlib==3.8.4\",\n    \"seaborn==0.13.2\",\n    \"nltk==3.8.1\",\n    \"rouge-score==0.1.2\",\n    \"sacrebleu==2.4.0\",\n    \"statsmodels==0.14.1\"\n  ],\n  \"Required Other language third-party packages\": [\n    \"No third-party dependencies required\"\n  ],\n  \"Logic Analysis\": [\n    [\n      \"main.py\",\n      \"CLI entrypoint. Parses mode (`prepare_data`, `train_stage1`, `train_stage2`, `train_stage3`, `eval`), loads Hydra config, seeds runtime, builds registry objects, and dispatches to scripts or trainer/evaluator classes. Imports: `src/core/config_schema.py`, `src/core/registry.py`, `src/core/utils.py`, `scripts/*.py`.\"\n    ],\n    [\n      \"src/core/config_schema.py\",\n      \"Defines typed config contracts (Pydantic dataclasses): data/model/train/eval/runtime. Central dependency for all modules. Must include defaults and validation (paths, batch sizes, crop sizes, stage compatibility).\"\n    ],\n    [\n      \"src/core/registry.py\",\n      \"Factory/registry for datasets, models, trainers, evaluators. Avoids hardcoded branching in `main.py`. Dependency hub for dependency injection.\"\n    ],\n    [\n      \"src/core/utils.py\",\n      \"Shared helpers: seeding, distributed init, checkpoint I/O, tensor/device helpers, path creation, JSON/CSV writers. Imported almost everywhere.\"\n    ],\n    [\n      \"src/core/logging_utils.py\",\n      \"Unified logging and metric sinks (console + JSON + CSV). Ensures all stages produce comparable artifacts for final paper tables.\"\n    ],\n    [\n      \"src/data/wsi_reader.py\",\n      \"OpenSlide wrapper for reading metadata and regions. Must normalize coordinate conventions and magnification assumptions (20x). Consumed by segmentation/tiling/feature extraction.\"\n    ],\n    [\n      \"src/data/segment_tissue.py\",\n      \"Implements HSV saturation threshold segmentation + blur/morphological closing + contour filtering. Output: binary tissue masks and contours.\"\n    ],\n    [\n      \"src/data/tile_wsi.py\",\n      \"Generates non-overlapping 512x512 patch coordinates over segmented tissue. Applies min tissue fraction filtering. Saves coordinate maps for deterministic reruns.\"\n    ],\n    [\n      \"src/data/extract_patch_features.py\",\n      \"Loads pretrained patch encoder (CONCH-like substitute if unavailable), extracts 768-d features for all tiles, stores feature tensor + coords in HDF5/PT. Hard dependency for all later stages.\"\n    ],\n    [\n      \"src/data/build_feature_grid.py\",\n      \"Maps sparse (x,y,feat) to 2D feature grids and background masks. Provides random crop APIs: 16x16 for stage1, configurable 64x64 for stage3.\"\n    ],\n    [\n      \"src/data/tissue_grouping.py\",\n      \"Clusters tissue contours by spatial proximity, removes tiny groups (<16 patches), writes group metadata. Required for stage1 ROI sampling policy.\"\n    ],\n    [\n      \"src/data/caption_report_processing.py\",\n      \"Text pipeline for stage2/3: cleaning, PHI stripping hooks, tokenization, optional rewrite interfaces. Should support cached JSONL pairs.\"\n    ],\n    [\n      \"src/data/datasets.py\",\n      \"Defines `Stage1Dataset`, `Stage2Dataset`, `Stage3Dataset`, and downstream eval datasets. Uses grid/caption/report artifacts; returns strongly typed batches.\"\n    ],\n    [\n      \"src/data/collate.py\",\n      \"Custom collate functions for variable-size grids, token padding, attention masks, and multimodal batch objects.\"\n    ],\n    [\n      \"src/models/alibi_2d.py\",\n      \"Core 2D ALiBi bias implementation from patch-grid Euclidean distances. Imported by Transformer attention blocks in TITAN encoder.\"\n    ],\n    [\n      \"src/models/titan_encoder.py\",\n      \"ViT-style slide encoder in feature space: MLP feature embed, 6-layer transformer, 12 heads, dim 768, MLP 3072, ALiBi option. Shared by stage1 and multimodal stages.\"\n    ],\n    [\n      \"src/models/ibot_heads.py\",\n      \"Student/teacher projection heads for iBOT-style objectives in feature space. Stage1-specific.\"\n    ],\n    [\n      \"src/models/text_modules.py\",\n      \"Text encoder + tokenizer wrappers (OpenCLIP/HF). Must expose normalized text embeddings and decoder inputs.\"\n    ],\n    [\n      \"src/models/coca_multimodal.py\",\n      \"CoCa-style multimodal model: TITAN image backbone + contrastive pooler + reconstruction query pooler + multimodal decoder. Used in stage2/3 training and report generation.\"\n    ],\n    [\n      \"src/models/losses.py\",\n      \"Implements iBOT distillation + masked loss, contrastive InfoNCE loss, captioning cross-entropy, combined stage2/3 losses with weights.\"\n    ],\n    [\n      \"src/train/base_trainer.py\",\n      \"Common loop engine: AMP, grad accumulation, DDP sync, checkpointing, early stop hooks, metric logging.\"\n    ],\n    [\n      \"src/train/stage1_trainer.py\",\n      \"Stage1 specifics: generates 2 global + 10 local views from 16x16 crops, teacher EMA update, iBOT loss compute, TITAN-V checkpoint export.\"\n    ],\n    [\n      \"src/train/stage2_trainer.py\",\n      \"Stage2 ROI-caption continual pretraining: loads TITAN-V, optimizes contrastive+caption losses at ROI scale, large effective batch logic.\"\n    ],\n    [\n      \"src/train/stage3_trainer.py\",\n      \"Stage3 WSI-report continual pretraining: 64x64 grid crops, lower LR/warmup for vision backbone, final TITAN export.\"\n    ],\n    [\n      \"src/eval/embed_api.py\",\n      \"Single embedding interface for slide/text/report features. Handles centering + L2 normalization shared across probe/retrieval/zero-shot.\"\n    ],\n    [\n      \"src/eval/linear_probe.py\",\n      \"scikit-learn logistic regression (L-BFGS), hyperparameter search over regularization grid, multiclass/binary metrics.\"\n    ],\n    [\n      \"src/eval/knn_probe.py\",\n      \"Prototype and kNN (k=20) evaluation on centered-normalized embeddings.\"\n    ],\n    [\n      \"src/eval/few_shot.py\",\n      \"Few-shot K={1,2,4,8,16,32}, repeated sampling (50 runs), both SimpleShot prototype and optional linear-probe few-shot variant.\"\n    ],\n    [\n      \"src/eval/zero_shot.py\",\n      \"Prompt ensemble encoding, cosine similarity logits, class prediction; outputs balanced accuracy/AUROC per dataset.\"\n    ],\n    [\n      \"src/eval/retrieval.py\",\n      \"Unimodal and cross-modal retrieval evaluators. Computes Acc@K, MVAcc@5, Recall@K, mean recall with FAISS/scipy distances.\"\n    ],\n    [\n      \"src/eval/report_generation.py\",\n      \"Beam-search generation (`num_beams=5`), computes METEOR/ROUGE-1/BLEU-1 against references.\"\n    ],\n    [\n      \"src/eval/survival.py\",\n      \"Cox PH linear model with fold protocol and c-index computation using scikit-survival.\"\n    ],\n    [\n      \"src/eval/statistics.py\",\n      \"Bootstrap CI/s.d. and optional GLMM/Tukey pairwise comparison wrappers for paper-level significance testing.\"\n    ],\n    [\n      \"src/baselines/mean_pooling.py\",\n      \"Strong unsupervised baseline over patch features. Reused in probe/retrieval pipelines.\"\n    ],\n    [\n      \"src/baselines/abmil.py\",\n      \"Supervised ABMIL baseline for comparative experiments.\"\n    ],\n    [\n      \"scripts/prepare_public_data.py\",\n      \"End-to-end artifact preparation script (segmentation -> tiling -> features -> grids -> split manifests). Must be idempotent.\"\n    ],\n    [\n      \"scripts/run_stage1.py\",\n      \"Thin orchestrator for stage1 training with config overrides and checkpoint resume.\"\n    ],\n    [\n      \"scripts/run_stage2.py\",\n      \"Orchestrates stage2 multimodal training from TITAN-V checkpoint.\"\n    ],\n    [\n      \"scripts/run_stage3.py\",\n      \"Orchestrates stage3 multimodal training from stage2 checkpoint.\"\n    ],\n    [\n      \"scripts/run_eval.py\",\n      \"Runs requested downstream tasks and aggregates outputs into a single report artifact.\"\n    ],\n    [\n      \"configs/*.yaml\",\n      \"All reproducibility-critical settings in versioned YAML: model sizes, data paths, stage hyperparams, eval toggles, and seed. No hardcoded constants in Python.\"\n    ],\n    [\n      \"tests/test_alibi_2d.py\",\n      \"Validates ALiBi bias shape/value monotonicity and attention integration.\"\n    ],\n    [\n      \"tests/test_feature_grid.py\",\n      \"Checks coordinate->grid correctness, masking, and crop determinism.\"\n    ],\n    [\n      \"tests/test_losses.py\",\n      \"Unit tests for iBOT/contrastive/caption losses and weighted composition.\"\n    ],\n    [\n      \"tests/test_eval_metrics.py\",\n      \"Metric sanity tests for AUROC, balanced accuracy, retrieval@K, generation metrics.\"\n    ]\n  ],\n  \"Task list\": [\n    \"configs/default.yaml\",\n    \"configs/data/public_repro.yaml\",\n    \"configs/model/titan_v.yaml\",\n    \"configs/model/titan_multimodal.yaml\",\n    \"configs/train/stage1_ibot.yaml\",\n    \"configs/train/stage2_coca.yaml\",\n    \"configs/train/stage3_coca.yaml\",\n    \"configs/eval/linear_probe.yaml\",\n    \"configs/eval/few_shot.yaml\",\n    \"configs/eval/zero_shot.yaml\",\n    \"configs/eval/retrieval.yaml\",\n    \"configs/eval/report_generation.yaml\",\n    \"configs/eval/survival.yaml\",\n    \"src/core/config_schema.py\",\n    \"src/core/utils.py\",\n    \"src/core/logging_utils.py\",\n    \"src/core/registry.py\",\n    \"src/data/wsi_reader.py\",\n    \"src/data/segment_tissue.py\",\n    \"src/data/tile_wsi.py\",\n    \"src/data/extract_patch_features.py\",\n    \"src/data/build_feature_grid.py\",\n    \"src/data/tissue_grouping.py\",\n    \"src/data/caption_report_processing.py\",\n    \"src/data/datasets.py\",\n    \"src/data/collate.py\",\n    \"src/models/alibi_2d.py\",\n    \"src/models/titan_encoder.py\",\n    \"src/models/ibot_heads.py\",\n    \"src/models/text_modules.py\",\n    \"src/models/coca_multimodal.py\",\n    \"src/models/losses.py\",\n    \"src/train/base_trainer.py\",\n    \"src/train/stage1_trainer.py\",\n    \"src/train/stage2_trainer.py\",\n    \"src/train/stage3_trainer.py\",\n    \"scripts/prepare_public_data.py\",\n    \"scripts/run_stage1.py\",\n    \"scripts/run_stage2.py\",\n    \"scripts/run_stage3.py\",\n    \"src/eval/embed_api.py\",\n    \"src/eval/linear_probe.py\",\n    \"src/eval/knn_probe.py\",\n    \"src/eval/few_shot.py\",\n    \"src/eval/zero_shot.py\",\n    \"src/eval/retrieval.py\",\n    \"src/eval/report_generation.py\",\n    \"src/eval/survival.py\",\n    \"src/eval/statistics.py\",\n    \"src/baselines/mean_pooling.py\",\n    \"src/baselines/abmil.py\",\n    \"scripts/run_eval.py\",\n    \"main.py\",\n    \"tests/test_alibi_2d.py\",\n    \"tests/test_feature_grid.py\",\n    \"tests/test_losses.py\",\n    \"tests/test_eval_metrics.py\"\n  ],\n  \"Full API spec\": \"\",\n  \"Shared Knowledge\": \"Global constants and conventions must be centralized in config: patch_size=512, train magnification=20x, stage1 region=16x16, stage1 views=(2 global 14x14 + 10 local 6x6), stage3 crop=64x64, embedding_dim=768, transformer_layers=6, heads=12, mlp_dim=3072. All modules must consume the same artifact formats (`features.h5`: coords+features, `grid.pt`: grid+mask, `pairs.jsonl`: slide/report/caption mappings, `splits.csv`: train/val/test/fold). `EmbeddingService` is shared by linear/few-shot/zero-shot/retrieval/survival to avoid inconsistent normalization. Metric definitions are shared in eval utils (balanced accuracy, weighted F1, AUROC, c-index, Acc@K, MVAcc@5, Recall@K, METEOR/ROUGE-1/BLEU-1). Reproducibility settings shared across scripts: deterministic seeds, fold definitions, bootstrap count (1000), run count for few-shot (50), and checkpoint naming (`titan_v.ckpt`, `titan_stage2.ckpt`, `titan_final.ckpt`).\",\n  \"Anything UNCLEAR\": \"Paper-critical details missing from provided text: full supplementary hyperparameters (optimizer betas, LR/WD schedules, warmup lengths, iBOT temperatures/centering/masking, stage2/3 loss weights), exact 2D ALiBi slope schedule, precise feature-space posterization augmentation, exact tissue-group clustering thresholds, and inaccessible internal datasets (Mass-340K and internal eval cohorts). Need decision: strict public-only reproduction vs institution-augmented reproduction.\"\n}\n[/CONTENT]"}, {"role": "user", "content": "You write elegant, modular, and maintainable code. Adhere to Google-style guidelines.\n\nBased on the paper, plan, design specified previously, follow the \"Format Example\" and generate the code. \nExtract the training details from the above paper (e.g., learning rate, batch size, epochs, etc.), follow the \"Format example\" and generate the code. \nDO NOT FABRICATE DETAILS \u2014 only use what the paper provides.\n\nYou must write `config.yaml`.\n\nATTENTION: Use '##' to SPLIT SECTIONS, not '#'. Your output format must follow the example below exactly.\n\n-----\n\n# Format Example\n## Code: config.yaml\n```yaml\n## config.yaml\ntraining:\n  learning_rate: ...\n  batch_size: ...\n  epochs: ...\n...\n```\n\n-----\n\n## Code: config.yaml\n"}, {"role": "assistant", "content": "## Code: config.yaml\n```yaml\n## config.yaml\nruntime:\n  python_version: \"3.9.16\"\n  pytorch_version: \"2.0.1\"\n  cuda_version: \"11.8\"\n\nhardware:\n  stage1:\n    gpus: 4\n    gpu_type: \"NVIDIA A100 80GB\"\n  stage2:\n    gpus: 8\n    gpu_type: \"NVIDIA A100 80GB\"\n  stage3:\n    gpus: 8\n    gpu_type: \"NVIDIA A100 80GB\"\n  downstream_eval:\n    gpus: 1\n    gpu_type: \"NVIDIA 3090 24GB\"\n\ndata:\n  wsi_patch_size_px: 512\n  magnification: \"20x\"\n  patch_feature_dim: 768\n  roi_region_size_px: 8192\n  roi_region_grid_size: [16, 16]\n  stage3_wsi_crop_grid_size: [64, 64]\n  stage3_wsi_crop_size_px: 32768\n\nmodel:\n  slide_encoder:\n    architecture: \"ViT in feature space\"\n    num_layers: 6\n    num_attention_heads: 12\n    head_dim: 64\n    embedding_dim: 768\n    mlp_hidden_dim: 3072\n    positional_encoding: \"2D ALiBi (Euclidean-distance based bias)\"\n  multimodal:\n    framework: \"CoCa\"\n    reconstruction_queries: 128\n    text_encoder_source: \"CONCHv1.5 pretrained text encoder\"\n    text_decoder_source: \"CONCHv1.5 pretrained multimodal decoder\"\n    text_encoder_layers: 12\n    text_decoder_layers: 12\n    text_embedding_dim: 768\n    text_hidden_dim: 3072\n\ntraining:\n  stage1_titan_v:\n    objective: \"iBOT (student-teacher distillation + masked image modeling) in feature space\"\n    epochs: 270\n    iterations: 91260\n    local_batch_size_per_gpu: 256\n    effective_batch_size: 1024\n    view_sampling:\n      global_views: 2\n      global_view_grid_size: [14, 14]\n      local_views: 10\n      local_view_grid_size: [6, 6]\n    augmentations:\n      - \"horizontal_flip\"\n      - \"vertical_flip\"\n      - \"feature_posterization\"\n    optimizer:\n      learning_rate: null\n      weight_decay: null\n      betas: null\n      scheduler: null\n      warmup: null\n  stage2_roi_caption_alignment:\n    objective: \"CoCa contrastive + generative alignment on ROI-caption pairs\"\n    num_pairs: 423122\n    local_batch_size_per_gpu: 196\n    gradient_accumulation_steps: 2\n    effective_batch_size: 3136\n    optimizer:\n      learning_rate: null\n      weight_decay: null\n      betas: null\n      scheduler: null\n      warmup: null\n  stage3_wsi_report_alignment:\n    objective: \"CoCa contrastive + generative alignment on WSI-report pairs\"\n    num_pairs: 182862\n    local_batch_size_per_gpu: 16\n    gradient_accumulation_steps: 2\n    effective_batch_size: 256\n    notes:\n      - \"Use smaller learning rate and weight decay for vision backbone\"\n      - \"Use slow warm-up for vision backbone\"\n    optimizer:\n      learning_rate: null\n      weight_decay: null\n      betas: null\n      scheduler: null\n      warmup: null\n\nevaluation:\n  linear_probe:\n    method: \"scikit-learn logistic regression (L-BFGS)\"\n    l2_grid:\n      count: 45\n      min: 1e-6\n      max: 10\n      spacing: \"log\"\n    max_iter: 500\n    few_shot_or_no_val_defaults:\n      l2: 1\n      max_iter: 1000\n  knn_probe:\n    k: 20\n    distance: \"euclidean\"\n    preprocess:\n      - \"center\"\n      - \"l2_normalize\"\n  few_shot:\n    shots: [1, 2, 4, 8, 16, 32]\n    runs: 50\n  retrieval:\n    slide_retrieval_k: [1, 3, 5]\n    cross_modal_recall_k: [1, 3, 5, 10]\n  report_generation:\n    decoding: \"beam_search\"\n    num_beams: 5\n    num_beam_groups: 1\n  survival:\n    model: \"linear Cox proportional hazards\"\n    package: \"scikit-survival\"\n\nnotes:\n  source_of_truth:\n    - \"Hyperparameters not listed above are not provided in the supplied paper text and are intentionally left null.\"\n    - \"Paper references supplementary tables for missing optimizer/scheduler specifics.\"\n```"}]