## Code: src/models/text_modules.py
```python
"""Text encoder and multimodal decoder modules for TITAN reproduction.

This module implements the design-locked interfaces:
- TextEncoder.__init__(name: str, vocab_size: int, max_len: int)
- TextEncoder.forward(input_ids: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor
- MultimodalDecoder.__init__(num_layers: int, embed_dim: int)
- MultimodalDecoder.forward(image_tokens: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor
- MultimodalDecoder.generate(
    image_tokens: torch.Tensor,
    num_beams: int,
    max_new_tokens: int,
  ) -> torch.Tensor

The implementation stays config-aligned with provided constants and avoids
introducing paper-unknown hyperparameters as hard requirements.
"""

from __future__ import annotations

from typing import List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F


# -----------------------------------------------------------------------------
# Config-locked constants from provided config.yaml.
# -----------------------------------------------------------------------------
_PATCH_SIZE_PX: int = 512
_MAGNIFICATION: str = "20x"
_FEATURE_DIM: int = 768

_TEXT_EMBED_DIM: int = 768
_TEXT_HIDDEN_DIM: int = 3072
_TEXT_ENCODER_LAYERS: int = 12
_TEXT_DECODER_LAYERS: int = 12
_NUM_HEADS: int = 12

_DEFAULT_VOCAB_SIZE: int = 32_768
_DEFAULT_MAX_LEN: int = 256
_DEFAULT_DROPOUT: float = 0.0
_DEFAULT_LAYER_NORM_EPS: float = 1.0e-6

_DEFAULT_PAD_TOKEN_ID: int = 0
_DEFAULT_BOS_TOKEN_ID: int = 2
_DEFAULT_EOS_TOKEN_ID: int = 3


class TextModulesError(RuntimeError):
    """Base exception for text module failures."""


class TextEncoderConfigError(TextModulesError):
    """Raised when text encoder configuration is invalid."""


class TextDecoderConfigError(TextModulesError):
    """Raised when multimodal decoder configuration is invalid."""


class TextEncoder(nn.Module):
    """Mask-aware Transformer text encoder.

    Output is a pooled and L2-normalized embedding in the shared 768-d space.
    """

    def __init__(
        self,
        name: str,
        vocab_size: int,
        max_len: int,
        embed_dim: int = _TEXT_EMBED_DIM,
        hidden_dim: int = _TEXT_HIDDEN_DIM,
        num_layers: int = _TEXT_ENCODER_LAYERS,
        num_heads: int = _NUM_HEADS,
        dropout: float = _DEFAULT_DROPOUT,
        pad_token_id: int = _DEFAULT_PAD_TOKEN_ID,
    ) -> None:
        """Initialize text encoder.

        Args:
            name: Encoder source/name marker for provenance.
            vocab_size: Token vocabulary size.
            max_len: Maximum supported sequence length.
            embed_dim: Embedding width (must align to 768 by default).
            hidden_dim: Transformer feedforward width.
            num_layers: Number of encoder layers.
            num_heads: Number of attention heads.
            dropout: Transformer dropout probability.
            pad_token_id: Padding token id.
        """
        super().__init__()

        if not isinstance(name, str) or not name.strip():
            raise TextEncoderConfigError("name must be a non-empty string.")
        if isinstance(vocab_size, bool) or not isinstance(vocab_size, int):
            raise TextEncoderConfigError("vocab_size must be an integer.")
        if isinstance(max_len, bool) or not isinstance(max_len, int):
            raise TextEncoderConfigError("max_len must be an integer.")
        if isinstance(embed_dim, bool) or not isinstance(embed_dim, int):
            raise TextEncoderConfigError("embed_dim must be an integer.")
        if isinstance(hidden_dim, bool) or not isinstance(hidden_dim, int):
            raise TextEncoderConfigError("hidden_dim must be an integer.")
        if isinstance(num_layers, bool) or not isinstance(num_layers, int):
            raise TextEncoderConfigError("num_layers must be an integer.")
        if isinstance(num_heads, bool) or not isinstance(num_heads, int):
            raise TextEncoderConfigError("num_heads must be an integer.")
        if not isinstance(dropout, (int, float)):
            raise TextEncoderConfigError("dropout must be numeric.")
        if isinstance(pad_token_id, bool) or not isinstance(pad_token_id, int):
            raise TextEncoderConfigError("pad_token_id must be an integer.")

        if vocab_size <= 0:
            raise TextEncoderConfigError("vocab_size must be > 0.")
        if max_len <= 0:
            raise TextEncoderConfigError("max_len must be > 0.")
        if embed_dim <= 0:
            raise TextEncoderConfigError("embed_dim must be > 0.")
        if hidden_dim <= 0:
            raise TextEncoderConfigError("hidden_dim must be > 0.")
        if num_layers <= 0:
            raise TextEncoderConfigError("num_layers must be > 0.")
        if num_heads <= 0:
            raise TextEncoderConfigError("num_heads must be > 0.")
        if not (0.0 <= float(dropout) <= 1.0):
            raise TextEncoderConfigError("dropout must be in [0, 1].")
        if pad_token_id < 0:
            raise TextEncoderConfigError("pad_token_id must be >= 0.")

        if embed_dim != _TEXT_EMBED_DIM:
            raise TextEncoderConfigError(
                f"embed_dim must be {_TEXT_EMBED_DIM} for TITAN alignment, got {embed_dim}."
            )
        if hidden_dim != _TEXT_HIDDEN_DIM:
            raise TextEncoderConfigError(
                f"hidden_dim must be {_TEXT_HIDDEN_DIM} per config, got {hidden_dim}."
            )
        if num_layers != _TEXT_ENCODER_LAYERS:
            raise TextEncoderConfigError(
                f"num_layers must be {_TEXT_ENCODER_LAYERS} per config, got {num_layers}."
            )
        if num_heads != _NUM_HEADS:
            raise TextEncoderConfigError(
                f"num_heads must be {_NUM_HEADS} per config, got {num_heads}."
            )
        if embed_dim % num_heads != 0:
            raise TextEncoderConfigError("embed_dim must be divisible by num_heads.")

        self.name: str = name.strip()
        self.vocab_size: int = int(vocab_size)
        self.max_len: int = int(max_len)
        self.embed_dim: int = int(embed_dim)
        self.hidden_dim: int = int(hidden_dim)
        self.num_layers: int = int(num_layers)
        self.num_heads: int = int(num_heads)
        self.dropout_p: float = float(dropout)
        self.pad_token_id: int = int(pad_token_id)

        self.patch_size_px: int = _PATCH_SIZE_PX
        self.magnification: str = _MAGNIFICATION
        self.feature_dim: int = _FEATURE_DIM

        self.token_embedding: nn.Embedding = nn.Embedding(
            num_embeddings=self.vocab_size,
            embedding_dim=self.embed_dim,
            padding_idx=self.pad_token_id,
        )
        self.position_embedding: nn.Parameter = nn.Parameter(
            torch.zeros(self.max_len, self.embed_dim, dtype=torch.float32)
        )

        encoder_layer: nn.TransformerEncoderLayer = nn.TransformerEncoderLayer(
            d_model=self.embed_dim,
            nhead=self.num_heads,
            dim_feedforward=self.hidden_dim,
            dropout=self.dropout_p,
            activation="gelu",
            layer_norm_eps=_DEFAULT_LAYER_NORM_EPS,
            batch_first=True,
            norm_first=True,
        )
        self.encoder: nn.TransformerEncoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=self.num_layers,
            norm=nn.LayerNorm(self.embed_dim, eps=_DEFAULT_LAYER_NORM_EPS),
        )
        self.output_norm: nn.LayerNorm = nn.LayerNorm(self.embed_dim, eps=_DEFAULT_LAYER_NORM_EPS)
        self.output_proj: nn.Linear = nn.Linear(self.embed_dim, self.embed_dim, bias=True)

    def forward(self, input_ids: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor:
        """Encode token ids into normalized text embeddings.

        Args:
            input_ids: Token ids of shape [B, L].
            attn_mask: Binary mask of shape [B, L], where 1 means valid token.

        Returns:
            Tensor of shape [B, 768] in shared multimodal space.
        """
        if not isinstance(input_ids, torch.Tensor):
            raise TypeError(f"input_ids must be torch.Tensor, got {type(input_ids).__name__}.")
        if not isinstance(attn_mask, torch.Tensor):
            raise TypeError(f"attn_mask must be torch.Tensor, got {type(attn_mask).__name__}.")

        if input_ids.ndim != 2:
            raise TextModulesError(
                f"input_ids must have shape [B, L], got {tuple(input_ids.shape)}."
            )
        if attn_mask.ndim != 2:
            raise TextModulesError(
                f"attn_mask must have shape [B, L], got {tuple(attn_mask.shape)}."
            )
        if tuple(input_ids.shape) != tuple(attn_mask.shape):
            raise TextModulesError(
                "input_ids and attn_mask must have the same shape. "
                f"Got {tuple(input_ids.shape)} vs {tuple(attn_mask.shape)}."
            )

        batch_size: int = int(input_ids.shape[0])
        seq_len: int = int(input_ids.shape[1])
        if batch_size <= 0 or seq_len <= 0:
            raise TextModulesError(
                f"Invalid input shapes: input_ids={tuple(input_ids.shape)}."
            )
        if seq_len > self.max_len:
            raise TextModulesError(
                f"Sequence length {seq_len} exceeds max_len={self.max_len}."
            )

        token_ids: torch.Tensor = input_ids.to(dtype=torch.long)
        valid_mask: torch.Tensor = attn_mask.to(dtype=torch.bool)

        token_embed: torch.Tensor = self.token_embedding(token_ids)
        pos_embed: torch.Tensor = self.position_embedding[:seq_len, :].unsqueeze(0)
        hidden: torch.Tensor = token_embed + pos_embed

        key_padding_mask: torch.Tensor = ~valid_mask
        hidden = self.encoder(hidden, src_key_padding_mask=key_padding_mask)
        hidden = self.output_norm(hidden)

        valid_float: torch.Tensor = valid_mask.to(dtype=hidden.dtype)
        pooled_num: torch.Tensor = (hidden * valid_float.unsqueeze(-1)).sum(dim=1)
        pooled_den: torch.Tensor = valid_float.sum(dim=1, keepdim=True).clamp_min(1.0)
        pooled: torch.Tensor = pooled_num / pooled_den

        pooled = self.output_proj(pooled)
        pooled = F.normalize(pooled, p=2.0, dim=-1, eps=1.0e-12)

        return pooled


class MultimodalDecoder(nn.Module):
    """CoCa-style decoder conditioned on image tokens.

    The decoder consumes image memory tokens and autoregressive text tokens,
    returning token logits for caption/report generation.
    """

    def __init__(
        self,
        num_layers: int = _TEXT_DECODER_LAYERS,
        embed_dim: int = _TEXT_EMBED_DIM,
        hidden_dim: int = _TEXT_HIDDEN_DIM,
        vocab_size: int = _DEFAULT_VOCAB_SIZE,
        max_len: int = _DEFAULT_MAX_LEN,
        num_heads: int = _NUM_HEADS,
        dropout: float = _DEFAULT_DROPOUT,
        pad_token_id: int = _DEFAULT_PAD_TOKEN_ID,
        bos_token_id: int = _DEFAULT_BOS_TOKEN_ID,
        eos_token_id: int = _DEFAULT_EOS_TOKEN_ID,
    ) -> None:
        """Initialize multimodal decoder.

        Args:
            num_layers: Number of Transformer decoder layers.
            embed_dim: Shared embedding dimension.
            hidden_dim: Decoder feedforward width.
            vocab_size: Decoder vocabulary size.
            max_len: Maximum decoder context length.
            num_heads: Number of attention heads.
            dropout: Decoder dropout probability.
            pad_token_id: Padding token id.
            bos_token_id: Beginning-of-sequence token id.
            eos_token_id: End-of-sequence token id.
        """
        super().__init__()

        if isinstance(num_layers, bool) or not isinstance(num_layers, int):
            raise TextDecoderConfigError("num_layers must be an integer.")
        if isinstance(embed_dim, bool) or not isinstance(embed_dim, int):
            raise TextDecoderConfigError("embed_dim must be an integer.")
        if isinstance(hidden_dim, bool) or not isinstance(hidden_dim, int):
            raise TextDecoderConfigError("hidden_dim must be an integer.")
        if isinstance(vocab_size, bool) or not isinstance(vocab_size, int):
            raise TextDecoderConfigError("vocab_size must be an integer.")
        if isinstance(max_len, bool) or not isinstance(max_len, int):
            raise TextDecoderConfigError("max_len must be an integer.")
        if isinstance(num_heads, bool) or not isinstance(num_heads, int):
            raise TextDecoderConfigError("num_heads must be an integer.")
        if not isinstance(dropout, (int, float)):
            raise TextDecoderConfigError("dropout must be numeric.")
        if isinstance(pad_token_id, bool) or not isinstance(pad_token_id, int):
            raise TextDecoderConfigError("pad_token_id must be an integer.")
        if isinstance(bos_token_id, bool) or not isinstance(bos_token_id, int):
            raise TextDecoderConfigError("bos_token_id must be an integer.")
        if isinstance(eos_token_id, bool) or not isinstance(eos_token_id, int):
            raise TextDecoderConfigError("eos_token_id must be an integer.")

        if num_layers <= 0:
            raise TextDecoderConfigError("num_layers must be > 0.")
        if embed_dim <= 0:
            raise TextDecoderConfigError("embed_dim must be > 0.")
        if hidden_dim <= 0:
            raise TextDecoderConfigError("hidden_dim must be > 0.")
        if vocab_size <= 0:
            raise TextDecoderConfigError("vocab_size must be > 0.")
        if max_len <= 0:
            raise TextDecoderConfigError("max_len must be > 0.")
        if num_heads <= 0:
            raise TextDecoderConfigError("num_heads must be > 0.")
        if not (0.0 <= float(dropout) <= 1.0):
            raise TextDecoderConfigError("dropout must be in [0, 1].")
        if pad_token_id < 0 or bos_token_id < 0 or eos_token_id < 0:
            raise TextDecoderConfigError("special token ids must be >= 0.")

        if embed_dim != _TEXT_EMBED_DIM:
            raise TextDecoderConfigError(
                f"embed_dim must be {_TEXT_EMBED_DIM} per config, got {embed_dim}."
            )
        if hidden_dim != _TEXT_HIDDEN_DIM:
            raise TextDecoderConfigError(
                f"hidden_dim must be {_TEXT_HIDDEN_DIM} per config, got {hidden_dim}."
            )
        if num_layers != _TEXT_DECODER_LAYERS:
            raise TextDecoderConfigError(
                f"num_layers must be {_TEXT_DECODER_LAYERS} per config, got {num_layers}."
            )
        if num_heads != _NUM_HEADS:
            raise TextDecoderConfigError(
                f"num_heads must be {_NUM_HEADS} per config, got {num_heads}."
            )
        if embed_dim % num_heads != 0:
            raise TextDecoderConfigError("embed_dim must be divisible by num_heads.")

        self.num_layers: int = int(num_layers)
        self.embed_dim: int = int(embed_dim)
        self.hidden_dim: int = int(hidden_dim)
        self.vocab_size: int = int(vocab_size)
        self.max_len: int = int(max_len)
        self.num_heads: int = int(num_heads)
        self.dropout_p: float = float(dropout)

        self.pad_token_id: int = int(pad_token_id)
        self.bos_token_id: int = int(bos_token_id)
        self.eos_token_id: int = int(eos_token_id)

        self.patch_size_px: int = _PATCH_SIZE_PX
        self.magnification: str = _MAGNIFICATION
        self.feature_dim: int = _FEATURE_DIM

        self.token_embedding: nn.Embedding = nn.Embedding(
            num_embeddings=self.vocab_size,
            embedding_dim=self.embed_dim,
            padding_idx=self.pad_token_id,
        )
        self.position_embedding: nn.Parameter = nn.Parameter(
            torch.zeros(self.max_len, self.embed_dim, dtype=torch.float32)
        )

        decoder_layer: nn.TransformerDecoderLayer = nn.TransformerDecoderLayer(
            d_model=self.embed_dim,
            nhead=self.num_heads,
            dim_feedforward=self.hidden_dim,
            dropout=self.dropout_p,
            activation="gelu",
            layer_norm_eps=_DEFAULT_LAYER_NORM_EPS,
            batch_first=True,
            norm_first=True,
        )
        self.decoder: nn.TransformerDecoder = nn.TransformerDecoder(
            decoder_layer=decoder_layer,
            num_layers=self.num_layers,
            norm=nn.LayerNorm(self.embed_dim, eps=_DEFAULT_LAYER_NORM_EPS),
        )
        self.output_proj: nn.Linear = nn.Linear(self.embed_dim, self.vocab_size, bias=True)

    def forward(self, image_tokens: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:
        """Decode text logits conditioned on image tokens.

        Args:
            image_tokens: Image memory tokens, shape [B, Q, D] or [Q, D].
            input_ids: Decoder token ids, shape [B, L] or [L].

        Returns:
            Logits tensor of shape [B, L, vocab_size].
        """
        memory: torch.Tensor = self._normalize_image_tokens(image_tokens)
        tokens: torch.Tensor = self._normalize_input_ids(input_ids)

        batch_size: int = int(tokens.shape[0])
        seq_len: int = int(tokens.shape[1])

        if int(memory.shape[0]) != batch_size:
            if int(memory.shape[0]) == 1 and batch_size > 1:
                memory = memory.expand(batch_size, -1, -1)
            else:
                raise TextModulesError(
                    "Batch mismatch between image_tokens and input_ids. "
                    f"Got image_tokens={tuple(memory.shape)}, input_ids={tuple(tokens.shape)}."
                )

        token_embed: torch.Tensor = self.token_embedding(tokens)
        pos_embed: torch.Tensor = self.position_embedding[:seq_len, :].unsqueeze(0)
        tgt: torch.Tensor = token_embed + pos_embed

        causal_mask: torch.Tensor = self._causal_mask(seq_len=seq_len, device=tgt.device, dtype=tgt.dtype)
        tgt_key_padding_mask: torch.Tensor = tokens.eq(self.pad_token_id)

        decoded: torch.Tensor = self.decoder(
            tgt=tgt,
            memory=memory,
            tgt_mask=causal_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=None,
        )

        logits: torch.Tensor = self.output_proj(decoded)
        return logits

    def generate(
        self,
        image_tokens: torch.Tensor,
        num_beams: int,
        max_new_tokens: int,
    ) -> torch.Tensor:
        """Generate sequences using beam search.

        Args:
            image_tokens: Image memory tokens [B, Q, D] or [Q, D].
            num_beams: Beam width (>=1).
            max_new_tokens: Maximum number of generated tokens (>=1).

        Returns:
            Generated token ids with shape [B, T_gen], padded to max generated
            length across the batch.
        """
        if isinstance(num_beams, bool) or not isinstance(num_beams, int):
            raise TextDecoderConfigError("num_beams must be an integer.")
        if isinstance(max_new_tokens, bool) or not isinstance(max_new_tokens, int):
            raise TextDecoderConfigError("max_new_tokens must be an integer.")
        if num_beams <= 0:
            raise TextDecoderConfigError("num_beams must be > 0.")
        if max_new_tokens <= 0:
            raise TextDecoderConfigError("max_new_tokens must be > 0.")

        memory: torch.Tensor = self._normalize_image_tokens(image_tokens)
        batch_size: int = int(memory.shape[0])

        generated_list: List[torch.Tensor] = []
        for batch_index in range(batch_size):
            memory_single: torch.Tensor = memory[batch_index : batch_index + 1]
            if num_beams == 1:
                seq: torch.Tensor = self._greedy_generate_single(
                    memory_single=memory_single,
                    max_new_tokens=max_new_tokens,
                )
            else:
                seq = self._beam_search_generate_single(
                    memory_single=memory_single,
                    num_beams=num_beams,
                    max_new_tokens=max_new_tokens,
                )
            generated_list.append(seq)

        max_len: int = max(int(seq.shape[0]) for seq in generated_list)
        output: torch.Tensor = torch.full(
            (batch_size, max_len),
            fill_value=self.pad_token_id,
            dtype=torch.long,
            device=memory.device,
        )
        for idx, seq in enumerate(generated_list):
            output[idx, : int(seq.shape[0])] = seq

        return output

    def _normalize_image_tokens(self, image_tokens: torch.Tensor) -> torch.Tensor:
        if not isinstance(image_tokens, torch.Tensor):
            raise TypeError(
                f"image_tokens must be torch.Tensor, got {type(image_tokens).__name__}."
            )
        if image_tokens.ndim == 2:
            if int(image_tokens.shape[1]) != self.embed_dim:
                raise TextModulesError(
                    f"image_tokens last dim must be {self.embed_dim}, got {int(image_tokens.shape[1])}."
                )
            memory: torch.Tensor = image_tokens.unsqueeze(0)
        elif image_tokens.ndim == 3:
            if int(image_tokens.shape[2]) != self.embed_dim:
                raise TextModulesError(
                    f"image_tokens last dim must be {self.embed_dim}, got {int(image_tokens.shape[2])}."
                )
            memory = image_tokens
        else:
            raise TextModulesError(
                "image_tokens must have shape [Q,D] or [B,Q,D]. "
                f"Got {tuple(image_tokens.shape)}."
            )

        if int(memory.shape[1]) <= 0:
            raise TextModulesError("image_tokens must contain at least one memory token.")

        return memory.to(dtype=torch.float32)

    def _normalize_input_ids(self, input_ids: torch.Tensor) -> torch.Tensor:
        if not isinstance(input_ids, torch.Tensor):
            raise TypeError(f"input_ids must be torch.Tensor, got {type(input_ids).__name__}.")

        if input_ids.ndim == 1:
            tokens: torch.Tensor = input_ids.unsqueeze(0)
        elif input_ids.ndim == 2:
            tokens = input_ids
        else:
            raise TextModulesError(
                "input_ids must have shape [L] or [B,L]. "
                f"Got {tuple(input_ids.shape)}."
            )

        batch_size: int = int(tokens.shape[0])
        seq_len: int = int(tokens.shape[1])
        if batch_size <= 0 or seq_len <= 0:
            raise TextModulesError(f"Invalid input_ids shape: {tuple(tokens.shape)}.")
        if seq_len > self.max_len:
            raise TextModulesError(
                f"Sequence length {seq_len} exceeds max_len={self.max_len}."
            )

        return tokens.to(dtype=torch.long)

    def _causal_mask(self, seq_len: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:
        # PyTorch Transformer expects additive mask with -inf above diagonal.
        mask: torch.Tensor = torch.full(
            (seq_len, seq_len),
            fill_value=float("-inf"),
            dtype=dtype,
            device=device,
        )
        mask = torch.triu(mask, diagonal=1)
        return mask

    def _greedy_generate_single(self, memory_single: torch.Tensor, max_new_tokens: int) -> torch.Tensor:
        device: torch.device = memory_single.device
        seq: torch.Tensor = torch.tensor(
            [self.bos_token_id], dtype=torch.long, device=device
        )

        with torch.inference_mode():
            for _ in range(max_new_tokens):
                decoder_input: torch.Tensor = seq.unsqueeze(0)
                logits: torch.Tensor = self.forward(image_tokens=memory_single, input_ids=decoder_input)
                next_token_logits: torch.Tensor = logits[:, -1, :]
                next_token: torch.Tensor = torch.argmax(next_token_logits, dim=-1)
                seq = torch.cat([seq, next_token.to(dtype=torch.long)], dim=0)
                if int(next_token.item()) == self.eos_token_id:
                    break

        return seq

    def _beam_search_generate_single(
        self,
        memory_single: torch.Tensor,
        num_beams: int,
        max_new_tokens: int,
    ) -> torch.Tensor:
        device: torch.device = memory_single.device

        beam_sequences: List[torch.Tensor] = [
            torch.tensor([self.bos_token_id], dtype=torch.long, device=device)
        ]
        beam_scores: torch.Tensor = torch.zeros((1,), dtype=torch.float32, device=device)
        beam_done: List[bool] = [False]

        with torch.inference_mode():
            for _ in range(max_new_tokens):
                candidate_seqs: List[torch.Tensor] = []
                candidate_scores: List[torch.Tensor] = []
                candidate_done: List[bool] = []

                for beam_index, sequence in enumerate(beam_sequences):
                    if beam_done[beam_index]:
                        candidate_seqs.append(sequence)
                        candidate_scores.append(beam_scores[beam_index])
                        candidate_done.append(True)
                        continue

                    decoder_input: torch.Tensor = sequence.unsqueeze(0)
                    logits: torch.Tensor = self.forward(image_tokens=memory_single, input_ids=decoder_input)
                    step_logits: torch.Tensor = logits[:, -1, :]
                    step_log_probs: torch.Tensor = F.log_softmax(step_logits, dim=-1).squeeze(0)

                    top_scores, top_indices = torch.topk(
                        step_log_probs,
                        k=min(num_beams, int(step_log_probs.shape[0])),
                        dim=-1,
                    )

                    for k in range(int(top_indices.shape[0])):
                        token_id: int = int(top_indices[k].item())
                        token_score: torch.Tensor = top_scores[k]
                        new_seq: torch.Tensor = torch.cat(
                            [sequence, torch.tensor([token_id], dtype=torch.long, device=device)],
                            dim=0,
                        )
                        new_score: torch.Tensor = beam_scores[beam_index] + token_score
                        is_done: bool = token_id == self.eos_token_id

                        candidate_seqs.append(new_seq)
                        candidate_scores.append(new_score)
                        candidate_done.append(is_done)

                if len(candidate_seqs) == 0:
                    break

                stacked_scores: torch.Tensor = torch.stack(candidate_scores, dim=0)
                keep_k: int = min(num_beams, int(stacked_scores.shape[0]))
                topk_scores, topk_indices = torch.topk(stacked_scores, k=keep_k, dim=0)

                next_beam_sequences: List[torch.Tensor] = []
                next_beam_done: List[bool] = []
                for rank in range(keep_k):
                    candidate_idx: int = int(topk_indices[rank].item())
                    next_beam_sequences.append(candidate_seqs[candidate_idx])
                    next_beam_done.append(candidate_done[candidate_idx])

                beam_sequences = next_beam_sequences
                beam_scores = topk_scores
                beam_done = next_beam_done

                if all(beam_done):
                    break

        if len(beam_sequences) == 0:
            return torch.tensor([self.bos_token_id], dtype=torch.long, device=device)

        # Length-normalized selection to avoid short-sequence bias.
        best_index: int = 0
        best_value: float = float("-inf")
        for idx, sequence in enumerate(beam_sequences):
            seq_len: int = max(1, int(sequence.shape[0]))
            normalized_score: float = float((beam_scores[idx] / float(seq_len)).item())
            if normalized_score > best_value:
                best_value = normalized_score
                best_index = idx

        return beam_sequences[best_index]


__all__ = [
    "TextModulesError",
    "TextEncoderConfigError",
    "TextDecoderConfigError",
    "TextEncoder",
    "MultimodalDecoder",
]
```